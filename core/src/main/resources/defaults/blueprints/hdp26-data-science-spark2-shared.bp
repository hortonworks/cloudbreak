{
  "inputs": [
    {
      "name":"S3_BUCKET",
      "referenceConfiguration":"s3.bucket"
    }
  ],
  "description": "Useful for data science with Spark and Zeppelin",
  "blueprint": {
    "Blueprints": {
      "blueprint_name": "hdp26-data-science-spark2-shared",
      "stack_name": "HDP",
      "stack_version": "2.6"
    },
    "configurations": [
      {
        "core-site": {
          "fs.trash.interval": "4320",
          "hadoop.security.group.mapping":"org.apache.hadoop.security.LdapGroupsMapping",
          "hadoop.security.group.mapping.ldap.url":"{{{ ldapConfig.connectionURL }}}",
          "hadoop.security.group.mapping.ldap.bind.user":"{{{ ldapConfig.bindDn }}}",
          "hadoop.security.group.mapping.ldap.bind.password":"{{{ ldapConfig.bindPassword }}}",
          "hadoop.security.group.mapping.ldap.userbase": "{{{ ldapConfig.userSearchBase }}}",
          "hadoop.security.group.mapping.ldap.search.filter.user": "(&(objectClass={{{ ldapConfig.userObjectClass }}})({{{ ldapConfig.userNameAttribute }}}={0}))",
          "hadoop.security.group.mapping.ldap.groupbase": "{{{ ldapConfig.groupSearchBase }}}",
          "hadoop.security.group.mapping.ldap.search.filter.group": "(objectClass={{{ ldapConfig.groupObjectClass }}})",
          "hadoop.security.group.mapping.ldap.search.attr.group.name": "{{{ ldapConfig.groupNameAttribute }}}",
          "hadoop.security.group.mapping.ldap.search.attr.member": "{{{ ldapConfig.groupMemberAttribute }}}"
        }
      },
      {
        "hdfs-site": {
          "dfs.namenode.safemode.threshold-pct": "0.99"
        }
      },
      {
        "mapred-site": {
          "mapreduce.job.reduce.slowstart.completedmaps": "0.7",
          "mapreduce.map.output.compress": "true",
          "mapreduce.output.fileoutputformat.compress": "true"
        }
      },
      {
        "yarn-site": {
          "yarn.acl.enable": "true"
        }
      },
      {
        "zeppelin-shiro-ini": {
          "properties": {
            "shiro_ini_content": "\n[users]\n# List of users with their password allowed to access Zeppelin.\n# To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections\n\n\n# Sample LDAP configuration, for user Authentication, currently tested for single Realm\n[main]\nldapRealm = org.apache.zeppelin.realm.LdapRealm\nldapRealm.contextFactory.url = {{{ ldapConfig.connectionURL }}}\nldapRealm.contextFactory.authenticationMechanism = simple\nldapRealm.contextFactory.systemUsername = {{{ ldapConfig.bindDn }}}\nldapRealm.contextFactory.systemPassword = {{{ ldapConfig.bindPassword }}}\nldapRealm.searchBase = {{{ ldapConfig.userSearchBase }}}\nldapRealm.userSearchAttributeName = {{{ ldapConfig.userNameAttribute }}}\nldapRealm.userObjectClass = {{{ ldapConfig.userObjectClass }}}\nldapRealm.memberAttribute = {{{ ldapConfig.groupMemberAttribute }}}\nldapRealm.groupObjectClass = {{{ ldapConfig.groupObjectClass }}}\nldapRealm.authorizationEnabled = true\nldapRealm.groupSearchBase = {{{ ldapConfig.groupSearchBase }}}\n\n\nsessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager\nsecurityManager.sessionManager = $sessionManager\n# 86,400,000 milliseconds = 24 hour\nsecurityManager.sessionManager.globalSessionTimeout = 86400000\nshiro.loginUrl = /api/login\n\n[urls]\n# anon means the access is anonymous.\n# authcBasic means Basic Auth Security\n# To enfore security, comment the line below and uncomment the next one\n/api/version = anon\n#/** = anon\n/** = authc\n"
          }
        }
      },
      {
        "spark-defaults": {
          "spark.hadoop.hive.llap.daemon.service.hosts": "@llap0",
          "spark.jars.packages": "com.hortonworks.spark:spark-llap-assembly_2.11:1.1.2-2.1",
          "spark.sql.hive.llap": "true",
          "spark.hadoop.hive.zookeeper.quorum": "<hive2 hs2 host>:2181",
          "spark.sql.hive.hiveserver2.jdbc.url": "jdbc:hive2://<hive2 hs2 host>:10501/default;transportMode=http;httpPath=cliservice"
        }
      },
      {
        "spark2-defaults": {
          "spark.hadoop.hive.llap.daemon.service.hosts": "@llap0",
          "spark.jars.packages": "com.hortonworks.spark:spark-llap-assembly_2.11:1.1.2-2.1",
          "spark.sql.hive.llap": "true",
          "spark.hadoop.hive.zookeeper.quorum": "<hive2 hs2 host>:2181",
          "spark.sql.hive.hiveserver2.jdbc.url": "jdbc:hive2://<hive2 hs2 host>:10501/default;transportMode=http;httpPath=cliservice"
        }
      },
      {
        "zeppelin-env": {
          "zeppelin_env_content": "\n# export JAVA_HOME=\nexport JAVA_HOME={{java64_home}}\n# export MASTER=                              # Spark master url. eg. spark://master_addr:7077. Leave empty if you want to use local mode.\nexport MASTER=yarn-client\nexport SPARK_YARN_JAR={{spark_jar}}\n# export ZEPPELIN_JAVA_OPTS                   # Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS=\"-Dspark.executor.memory=8g -Dspark.cores.max=16\"\nexport ZEPPELIN_JAVA_OPTS=\"-Dhdp.version={{full_stack_version}} -Dspark.executor.memory={{executor_mem}} -Dspark.executor.instances={{executor_instances}} -Dspark.yarn.queue={{spark_queue}}\"\n# export ZEPPELIN_MEM                         # Zeppelin jvm mem options Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_MEM                    # zeppelin interpreter process jvm mem options. Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n# export ZEPPELIN_INTP_JAVA_OPTS              # zeppelin interpreter process jvm options.\n# export ZEPPELIN_SSL_PORT                    # ssl port (used when ssl environment variable is set to true)\n\n# export ZEPPELIN_LOG_DIR                     # Where log files are stored.  PWD by default.\nexport ZEPPELIN_LOG_DIR={{zeppelin_log_dir}}\n# export ZEPPELIN_PID_DIR                     # The pid files are stored. ${ZEPPELIN_HOME}/run by default.\nexport ZEPPELIN_PID_DIR={{zeppelin_pid_dir}}\n# export ZEPPELIN_WAR_TEMPDIR                 # The location of jetty temporary directory.\n# export ZEPPELIN_NOTEBOOK_DIR                # Where notebook saved\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN         # Id of notebook to be displayed in homescreen. ex) 2A94M5J1Z\n# export ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE    # hide homescreen notebook from list when this value set to \"true\". default \"false\"\n# export ZEPPELIN_NOTEBOOK_S3_BUCKET          # Bucket where notebook saved\n# export ZEPPELIN_NOTEBOOK_S3_ENDPOINT        # Endpoint of the bucket\n# export ZEPPELIN_NOTEBOOK_S3_USER            # User in bucket where notebook saved. For example bucket/user/notebook/2A94M5J1Z/note.json\n# export ZEPPELIN_IDENT_STRING                # A string representing this instance of zeppelin. $USER by default.\n# export ZEPPELIN_NICENESS                    # The scheduling priority for daemons. Defaults to 0.\n# export ZEPPELIN_INTERPRETER_LOCALREPO       # Local repository for interpreter's additional dependency loading\n# export ZEPPELIN_NOTEBOOK_STORAGE            # Refers to pluggable notebook storage class, can have two classes simultaneously with a sync between them (e.g. local and remote).\n# export ZEPPELIN_NOTEBOOK_ONE_WAY_SYNC       # If there are multiple notebook storages, should we treat the first one as the only source of truth?\n# export ZEPPELIN_NOTEBOOK_PUBLIC             # Make notebook public by default when created, private otherwise\nexport ZEPPELIN_INTP_CLASSPATH_OVERRIDES=\"{{external_dependency_conf}}\"\n\n#### Spark interpreter configuration ####\n\n## Use provided spark installation ##\n## defining SPARK_HOME makes Zeppelin run spark interpreter process using spark-submit\n##\n# export SPARK_HOME                           # (required) When it is defined, load it instead of Zeppelin embedded Spark libraries\n#export SPARK_HOME={{spark_home}}\n# export SPARK_SUBMIT_OPTIONS                 # (optional) extra options to pass to spark submit. eg) \"--driver-memory 512M --executor-memory 1G\".\n# export SPARK_APP_NAME                       # (optional) The name of spark application.\n\nexport SPARK_SUBMIT_OPTIONS=\"--repositories http://repo.hortonworks.com/content/groups/public/\"\nexport SPARK_PRINT_LAUNCH_COMMAND=true\n\n## Use embedded spark binaries ##\n## without SPARK_HOME defined, Zeppelin still able to run spark interpreter process using embedded spark binaries.\n## however, it is not encouraged when you can define SPARK_HOME\n##\n# Options read in YARN client mode\n# export HADOOP_CONF_DIR                      # yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\n# Pyspark (supported with Spark 1.2.1 and above)\n# To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI\n# export PYSPARK_PYTHON                       # path to the python command. must be the same path on the driver(Zeppelin) and all workers.\n# export PYTHONPATH\n\nexport PYTHONPATH=\"${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip\"\nexport SPARK_YARN_USER_ENV=\"PYTHONPATH=${PYTHONPATH}\"\n\n## Spark interpreter options ##\n##\n# export ZEPPELIN_SPARK_USEHIVECONTEXT        # Use HiveContext instead of SQLContext if set true. true by default.\n# export ZEPPELIN_SPARK_CONCURRENTSQL         # Execute multiple SQL concurrently if set true. false by default.\n# export ZEPPELIN_SPARK_IMPORTIMPLICIT        # Import implicits, UDF collection, and sql if set true. true by default.\n# export ZEPPELIN_SPARK_MAXRESULT             # Max number of Spark SQL result to display. 1000 by default.\n# export ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE       # Size in characters of the maximum text message to be received by websocket. Defaults to 1024000\n\n\n#### HBase interpreter configuration ####\n\n## To connect to HBase running on a cluster, either HBASE_HOME or HBASE_CONF_DIR must be set\n\n# export HBASE_HOME=                          # (require) Under which HBase scripts and configuration should be\n# export HBASE_CONF_DIR=                      # (optional) Alternatively, configuration directory can be set to point to the directory that has hbase-site.xml\n\n# export ZEPPELIN_IMPERSONATE_CMD             # Optional, when user want to run interpreter as end web user. eg) 'sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c '\n"
        }
      }
    ],
    "host_groups": [
      {
        "name": "master",
        "configurations": [],
        "components": [
          {
            "name": "APP_TIMELINE_SERVER"
          },
          {
            "name": "HDFS_CLIENT"
          },
          {
            "name": "HISTORYSERVER"
          },
          {
            "name": "JOURNALNODE"
          },
          {
            "name": "MAPREDUCE2_CLIENT"
          },
          {
            "name": "METRICS_COLLECTOR"
          },
          {
            "name": "METRICS_MONITOR"
          },
          {
            "name": "NAMENODE"
          },
          {
            "name": "RESOURCEMANAGER"
          },
          {
            "name": "SECONDARY_NAMENODE"
          },
          {
            "name": "LIVY_SERVER"
          },
          {
            "name": "SPARK2_CLIENT"
          },
          {
            "name": "SPARK2_JOBHISTORYSERVER"
          },
          {
            "name": "SPARK_CLIENT"
          },
          {
            "name": "SPARK_JOBHISTORYSERVER"
          },
          {
            "name": "TEZ_CLIENT"
          },
          {
            "name": "YARN_CLIENT"
          },
          {
            "name": "ZEPPELIN_MASTER"
          },
          {
            "name": "ZOOKEEPER_CLIENT"
          },
          {
            "name": "ZOOKEEPER_SERVER"
          }
        ],
        "cardinality": "1"
      },
      {
        "name": "worker",
        "configurations": [],
        "components": [
          {
            "name": "SPARK2_CLIENT"
          },
          {
            "name": "SPARK_CLIENT"
          },
          {
            "name": "TEZ_CLIENT"
          },
          {
            "name": "DATANODE"
          },
          {
            "name": "METRICS_MONITOR"
          },
          {
            "name": "NODEMANAGER"
          }
        ],
        "cardinality": "1+"
      },
      {
        "name": "compute",
        "configurations": [],
        "components": [
          {
            "name": "SPARK2_CLIENT"
          },
          {
            "name": "SPARK_CLIENT"
          },
          {
            "name": "TEZ_CLIENT"
          },
          {
            "name": "METRICS_MONITOR"
          },
          {
            "name": "NODEMANAGER"
          }
        ],
        "cardinality": "1+"
      }
    ]
  }
}