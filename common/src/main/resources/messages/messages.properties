test.message=Hi my dear friend
stack.image.setup=Setting up CDP image
stack.image.fallback=Could not start cluster with Azure Marketplace image. Now attempting to fall back to VHD image
stack.image.marketplace.error=Marketplace error: {0}
stack.upscale.image.fallback=Could not add instance(s) with Azure Marketplace image. Now attempting to fall back to VHD image. {0}
stack.upscale.adjustment.type.fallback=Upscale operation will fall back to 'EXACT' adjustment type due to stability reasons.
stack.infrastructure.bootstrap=Bootstrapping cluster
stack.infrastructure.disk.mount=Mounting attached disks
stack.infrastructure.starting=Infrastructure is now starting
stack.infrastructure.started=Infrastructure successfully started
stack.infrastructure.stopping=Infrastructure is now stopping
stack.infrastructure.stopped=Infrastructure successfully stopped
stack.delete.in.progress=Terminating the cluster and its infrastructure
stack.pre.delete.in.progress=Pre-terminating the cluster and its infrastructure
stack.delete.completed=The cluster and its infrastructure have successfully been terminated
stack.forced.delete.completed=The cluster and its infrastructure have been force terminated in CloudBreak. You may need to terminate the cluster manually at the cloud provider.
stack.adding.instances=Adding {0} new instance(s) to host group: {1}. Adjustment type: {2}. Threshold: {3}
stack.gateway.certificate.create.skipped=The generation of valid certificate has been failed, installation of your cluster is continuing with a generated self-signed certificate.
stack.metadata.extend.with.count=Infrastructure metadata extension finished with failures, {0} new instances was created instead of {1}
stack.removing.instance=Removing instance from the infrastructure
stack.removing.instance.finished=Instance removed
stack.removing.instance.failed=Failed to remove instance
stack.mount.disks.on.new.hosts=Mounting disks on new nodes
stack.downscale.instances=Removing instance(s): {0}
stack.downscale.success=Removed instance(s): {0}
stack.downscale.failed=Cluster infrastructure downscale failed: {0}
stack.upscale.quota.issue=You have reached your quota limit on provider. Error message: {0}
stack.verticalscale.issue=Vertical Scale was not successfully executed. Error message: {0}
stack.select.for.downscale=Selected node(s) for downscale: {0}
stack.stop.requested=Cluster infrastructure stop requested
stack.provisioning=Creating infrastructure
stack.infrastructure.time=Infrastructure creation took {0} seconds
stack.setup.time=Image preparation took {0} seconds
stack.setup.start=Image preparation started
stack.infrastructure.subnets.updating=Updating allowed subnets
stack.infrastructure.subnets.updated=Allowed subnets updated
stack.infrastructure.update.failed=Stack update failed. {0}
stack.infrastructure.create.failed=Infrastructure creation failed. Reason: {0}
stack.instance.metadata.restored=Repair failed for instance, metadata restored for host: {0}
stack.infrastructure.rollback=Infrastructure rollback. Reason: {0}
stack.infrastructure.rollback.failed=Infrastructure rollback failed. Reason: {0}
stack.infrastructure.delete.failed=Infrastructure termination failed. Reason: {0}
stack.infrastructure.start.failed=Infrastructure start failed. Reason: {0}
stack.infrastructure.stop.failed=Infrastructure stop failed. Reason: {0}
stack.repair.detection.started=Started stack repair. Detecting unhealthy instances.
stack.repair.attempting=Found unhealthy instances, attempting to repair stack.
stack.repair.complete.clean=Completed stack repair. No unhealthy instances found.
stack.repair.failed=Failed to repair cluster. Reason: {0}
stack.repair.triggered=Stack repair triggered.
stack.datalake.update=Update cluster due to changed datalake
stack.datalake.update.finished=Update cluster finished
stack.datalake.update.failed=Failed to update the cluster. {0}
stack.infrastructure.create.rollback=Could not create {0} instance(s), node count is decreased on group {1}. Reason: {2}
stack.cleanup.service.trigger.sync=Couldn't retrieve the cluster's status, starting to sync.
stack.diagnostics.salt.validation.running=Validating salt minion connectivity before initializing diagnostics collection. Esxclude hosts: {0}
stack.diagnostics.salt.validation.running.skip.unresponsive=Validating salt minion connectivity before initializing diagnostics collection (skip unresponsive hosts). Initially excluded hosts: {0}
stack.diagnostics.salt.pillar.update.running=Update telemetry related salt configurations.
stack.diagnostics.salt.state.update.running=Update telemetry related salt states.
stack.diagnostics.preflight.check.running=Running pre-flight checks.
stack.diagnostics.preflight.check.finished={0}Pre-flight {1} check result: {2}
stack.diagnostics.vm.preflight.check.running=Running pre-flight checks on VM nodes.
stack.diagnostics.telemetry.upgrade.running=Running on-the-fly telemetry upgrade.
stack.diagnostics.init.running=Diagnostics collection initialization in progress. Hosts: {0}, Excluded hosts: {1}, Host-groups: {2}
stack.diagnostics.ensure.machine.user=Check availability of UMS resources for diagnostics.
stack.diagnostics.collection.running=Diagnostics collection in progress. Destination for logs: {0}
stack.diagnostics.upload.running=Diagnostics uploading in progress. {0}
stack.diagnostics.cleanup.running=Diagnostics cleanup in progress
stack.diagnostics.collection.finished=Diagnostics collection finished
stack.diagnostics.collection.failed=Diagnostics collection failed. Reason: {0}
stack.cm.diagnostics.init.running=CM based diagnostics collection initialization in progress.
stack.cm.diagnostics.collection.running=CM based diagnostics collection in progress. Destination for diagnostics data: {0}
stack.cm.diagnostics.upload.running=CM based diagnostics uploading in progress. {0}
stack.cm.diagnostics.cleanup.running=CM based diagnostics cleanup in progress
stack.cm.diagnostics.collection.finished=CM based Diagnostics collection finished
stack.cm.diagnostics.collection.failed=CM based diagnostics collection failed. Reason: {0}

stack.cm.mixed.package.versions.failed=This cluster is currently using Cloudera Manager {0} and Cloudera Runtime {1}, which is a combination not certified by Cloudera. This is most likely the result of an incomplete or failed upgrade. Please check the Event History for details. Using this combination is unadvised and might lead to unexpected errors. We recommend using the Retry feature to retry the upgrade of Cloudera Runtime to version {2}, continuing from the failed step of the previous upgrade operation.
stack.cm.mixed.package.versions.failed.no.candidate=This cluster is currently using Cloudera Manager {0} and Cloudera Runtime {1}, which is a combination not certified by Cloudera. This is most likely the result of an incomplete or failed upgrade. Please check the Event History for details. Using this combination is unadvised and might lead to unexpected errors. Please contact the Cloudera support for further actions.
stack.cm.mixed.package.versions.failed.multiple=This cluster is currently using Cloudera Manager {0} and Cloudera Runtime {1}, which is a combination not certified by Cloudera. This is most likely the result of an incomplete or failed upgrade. Please check the Event History for details. Using this combination is unadvised and might lead to unexpected errors. Please continue upgrading them to a certified combination, like {2}. For other possible combinations please visit the image catalog at {3}.
stack.cm.mixed.package.versions.newer.failed=This cluster is currently using Cloudera Manager {0} and Cloudera Runtime {1}, which is a combination not certified by Cloudera. This is most likely the result of an incomplete or failed upgrade. Please check the Event History for details. Using this combination is unadvised and might lead to unexpected errors. The following components are newer {2} then the expected versions {3}. Please use recovery or contact the Cloudera support.

cluster.starting=Starting cluster
cluster.dns.update.finished=DNS entries have been updated for the cluster
cluster.building=Installing CDP services
cluster.built=CDP services have been installed
configure.policy=Configure FISMA policies for Cloudera Manager has been completed.
recovery.finished=Cluster recovery has been completed
ambari.cluster.created=Cloudera Manager cluster created
cluster.started=Cluster has been started
cluster.stopping=Stopping cluster
cluster.stopped=Cluster stopped
cluster.termination.started=Cluster termination has been started
cluster.provision.idbrokerhost.cloudstorage.vmvalidation.failed=Validation of the cluster's cloud storage access for logging failed on the IDBroker host.

cluster.regenerate.keytabs=Regenerating kerberos keytabs on cluster
cluster.decommission.failed.force.delete.continue=Error during cluster decommission but force delete will continue. Reason: {0}
cluster.stop.management.server.started=Stopping cluster management server
cluster.start.management.server.started=Starting cluster management server
cluster.single.master.repair.reinstall.components=Reinstalling components on cluster management server
cluster.restart.all.started=Restarting all components on all hosts
cluster.start.components.started=Starting components
cluster.stop.components.started=Stopping all cluster components

cluster.certificate.reissue=Certificate renewal of the cluster has been started. Reissuing the certificate.
cluster.certificate.redeploy=The redeployment of the reissued certificate to the cluster has been started.
cluster.certificate.renewal.finished=Renewal of the cluster's certificate finished.
cluster.certificate.renewal.failed=Renewal of the cluster's certificate failed: '{0}'.

cluster.certificates.rotation.started=Certificates rotation of the cluster has been started.
cluster.host.certificates.rotation=Host certificates rotation of the cluster has been started.
cluster.certificates.rotation.finished=Rotation of the cluster's certificates have finished.
cluster.certificates.rotation.failed=Rotations of the cluster's certificates have failed: '{0}'.

cluster.manager.server.restarting=Restarting Cluster Manager Server
cluster.services.restarting=Restarting Cluster services

cluster.scaling.stopstart.upscale.init=Scaling up (via instance start) for host group: {0}.
cluster.scaling.stopstart.upscale.identifyrecoverycandidates=Identifying upscale recovery candidates for host group: {0}
cluster.scaling.stopstart.upscale.starting=Scaling up (via instance start) {0} instances for host group: {1}.
cluster.scaling.stopstart.upscale.starting.identifiedrecoverycandidates=Scaling up (via instance start) {0} instances for host group: {1}. Identified {2} recovery candidate(s): [{3}]
cluster.scaling.stopstart.upscale.nodes.started=Started Instances: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.nodes.started.withrecoverycandidates=Started Instances: count={0}, instanceId(s): [{1}], existing started instances: count={2}, instanceId(s): [{3}]
cluster.scaling.stopstart.upscale.nodes.notstarted=Some instances did not reach the desired 'STARTED' state, and may need attention: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.inadequate.nodes=Could not find adequate nodes to upscale(start) in hostGroup: {0}. Additional nodes may need to be added to the cluster. DesiredCount={1}, UpscaleCount={2}
cluster.scaling.stopstart.upscale.commissioning=Commissioning services. Host group: {0}, InstanceCount={1}, hostname(s): [{2}]
cluster.scaling.stopstart.upscale.commissioning2=Commissioning services. Host group: {0}, startedInstanceCount={1}, hostname(s): [{2}] recoveryCandidatesCount={3}, hostname(s): [{4}]
cluster.scaling.stopstart.upscale.exclude.lost.nodes=Found {0} disconnected host(s): [{1}]. Excluding them from the upscale operation.
cluster.scaling.stopstart.upscale.couldnotcommission=Could not commission services on all nodes. Some nodes may need attention: instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.upscale.waiting.hoststart=Waiting for {0} cm-host(s) to start before commissioning services
cluster.scaling.stopstart.upscale.cm.timeout=Failed while waiting for {0} nodes to move into health state in upscaling. MissingNodeCount={1}, MissingNode(s): [{2}]
cluster.scaling.stopstart.upscale.cmhostsstarted={0} cm-host(s) started
cluster.scaling.stopstart.upscale.start.failed=Failed while attempting to start instances. Some instances may need attention, and may be in STARTED state without services running. Refer to previous messages. Potential instances: instanceCount={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.commission.failed=Failed while attempting to commission services. Some instances may be STARTED with services not running. See previous events for list of started instances. Some hosts may need attention. instanceCount={0}, host(s): [{1}]
cluster.scaling.stopstart.upscale.finished=Scaled up (via instance start) host group: {0}. Instance details: count={1}, hostname(s): {2}
cluster.scaling.stopstart.upscale.failed=Failed to upscale (via instance start). Reason: {0}

cluster.scaling.stopstart.downscale.init=Scaling down (via instance stop) for host group: {0}
cluster.scaling.stopstart.downscale.identifyrecoverycandidates=Identifying downscale recovery candidates for host group: {0}
cluster.scaling.stopstart.downscale.starting=Scaling down (via instance stop) {0} node(s) from host group: {1}. Decommissioning services on hosts: [{2}]
cluster.scaling.stopstart.downscale.starting.identifiedrecoverycandidates=Scaling down (via instance stop) {0} node(s) from host group {1}. Decommissioning services on hosts: [{2}]. Identified {3} recovery candidates: [{4}]
cluster.scaling.stopstart.downscale.couldnotdecommission=Could not decommission services on all nodes. Some nodes may need attention: instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.downscale.enteringcmmaintmode=Putting {0} cm-host(s) into maintenance mode
cluster.scaling.stopstart.downscale.enteredcmmaintmode=Finished putting {0} cm-host(s) into maintenance mode
cluster.scaling.stopstart.downscale.exclude.lost.nodes=Found {0} disconnected host(s): [{1}]. Excluding them from the downscale operation.
cluster.scaling.stopstart.downscale.nodes.stopping=Stopping {0} node(s) from host group {1}. Hosts: [{2}]
cluster.scaling.stopstart.downscale.nodes.stopped=Stopped Instances: count={0}, instanceId(s): [{1}]
instances.restart.started=Restarting {0} node(s). InstanceId(s): [{1}]
instances.restart.finished=Restart of Instance(s) Finished. Successfully Restarted {0} Instances(s) InstanceId(s) [{1}]. Failed to Restart {2} Instances(s). InstanceId(s) [{3}]
instances.restart.failed=Failed to Restart instances
cluster.scaling.stopstart.downscale.nodes.notstopped=Some instances did not reach the desired 'STOPPED' state, and may need attention: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.downscale.decommission.failed=Failed while attempting to decommission services. Some nodes may need attention. instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.downscale.stop.failed=Failed while attempting to stop instances. Some instances may be in decommissioned state on CM. See previous events for details on such instances. Some instances may need attention on the cloud provider. instanceCount={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.downscale.finished=Scaled down (via instance stop) host group: {0}. Instance details: count={1}, instance(s): {2}
cluster.scaling.stopstart.downscale.failed=Failed to downscale (via instance stop). Reason: {0}

cluster.scaling.upscale.failed=Upscaling failed for nodes [{0}], possible cause can be a failed/timed out Cloudera Manager operation, please check Cloudera Manager UI for further details or contact support.
cluster.scaling.cm.memory.warning=Cloudera Manager is low on memory. Based on the current node count {0} GB memory is needed for Cloudera Manager. The current instance has {1} GB of memory, and we can assign 25% of it to Cloudera Manager. To avoid issues please scale vertically the node where Cloudera Manager is running.

cluster.scaling.up=Scaling up host group: {0}
cluster.re.register.with.cluster.proxy=Re-registering with Cluster Proxy service
cluster.scaled.up=Scaled up host group: {0}
cluster.scaling.down=Scaling down host group: {0}
cluster.scaling.down.zombie.nodes=Scaling down Zombie nodes from host group: {0}
cluster.scaled.down=Scaled down host group: {0}
cluster.scaled.down.none=0 instance has been scaled down
cluster.run.containers=Starting cluster containers
cluster.run.services=Starting Cloudera Manager
cluster.reset=Resetting cluster
ambari.cluster.configuring.security=Configuring Cloudera Manager cluster security
ambari.cluster.configured.security=Cloudera Manager cluster security configured
cluster.start.requested=Cluster start requested
cluster.changing.credential=Changing cluster authentication
cluster.changed.credential=Cluster authentication changed
cluster.change.credentail.failed=Unable to change cluster manager password.
cluster.changing.verticalscale=Vertical scaling started on group {0} from instance type {1} to {2}.
cluster.changed.verticalscale=Vertical scaling completed on group {0} from instance type {1} to {2}.
cluster.increasing.rootvolume=A process to increase the root volume to {0} GB has been initiated on group {1}.
cluster.increased.rootvolume=The root volume has been successfully increased to {0} GB for group {1}.
cluster.change.verticalscale.failed=Vertical scaling failed on group {0} from instance type {1} to {2}: {3}.
cluster.start.failed=Cluster could not be started. Reason: {0}
cluster.stop.failed=Cluster could not be stopped. Reason: {0}
cluster.create.failed={0}
ambari.cluster.configure.security.failed=Failed to enable security on Cloudera Manager cluster. Reason: {0}
cluster.scaling.failed=New node(s) could not be {0} the cluster. Reason {1}
cluster.scaling.partially.failed=Scaling up was not fully successful. Some of the new node(s) could not be {0} the cluster. Reason {1}
ambari.cluster.mr.smoke.failed=Warning: MapReduce smoke test failed, check your cluster''s configurations!
ambari.cluster.install.failed=Cluster installation failed to complete, please check the Cloudera Manager UI for more details. You can try to reinstall the cluster with a different cluster definition or fix the failures in Cloudera Manager and sync the cluster with Cloudbreak later.
ambari.cluster.upscale.failed=Cluster upscale failed to complete, please check the Cloudera Manager UI for more details. You can try to fix the failures in Cloudera Manager and sync the cluster with Cloudbreak later.
ambari.cluster.prepare.dekerberizing.failed=Prepare cluster to dekerberizing failed to complete, please check the Cloudera Manager UI for more details.
ambari.cluster.prepare.dekerberizing.error=The de-registration of Kerberos principals couldn''t be done. The Cloudera Manager server should run and be reachable by Cloudbreak.
ambari.cluster.disable.kerberos.failed=Cluster dekerberizing failed to complete, please check the Cloudera Manager UI for more details.
ambari.cluster.host.join.failed=Error while waiting for hosts to connect.
ambari.cluster.delete.completed=Cluster {0} was terminated.
ambari.cluster.delete.failed=Termination of cluster failed. Reason: {0}
ambari.cluster.services.init.failed=Initializing Cloudera Manager services failed.
ambari.regenerate.kerberos.keytabs.failed=Regenerating kerberos keytabs on Cloudera Manager failed.
ambari.cluster.services.start.failed=Starting Cloudera Manager services failed.
ambari.cluster.services.stop.failed=Stopping Cloudera Manager services failed.
cluster.ambari.cluster.services.starting=Starting Cloudera Manager services.
cluster.ambari.cluster.services.started=Cloudera Manager services have been started.
cluster.ambari.cluster.services.stopping=Stopping Cloudera Manager services.
cluster.ambari.cluster.services.stopped=Cloudera Manager services have been stopped.
cluster.autorecovery.requested.host=Cluster autorecovery requested for node {0}
cluster.autorecovery.requested.cluster=Cluster autorecovery requested, failed nodes: {0}
cluster.manualrecovery.requested=Cluster instance replacement requested on entities: {0}
cluster.manualrecovery.could.not.start=Cluster instance replacement requested on entities could not start for nodes due to the following reason: {0}
cluster.manualrecovery.no.nodes.to.recover=Cluster instance replacement requested on nodes which do not require recovery or recovery is not possible.
cluster.failednodes.reported.cluster=Cloudera Manager reported health issues with node(s): {0}
cluster.failednodes.reported.host=Cloudera Manager reported health issues with node {0}. Node could be UNHEALTHY or down. Please check CM UI for further information.
cluster.recoverednodes.reported.cluster=Cloudera Manager reported that node(s) {0} status became HEALTHY.
cluster.recoverednodes.reported.host=Cloudera Manager reported that node {0} status became HEALTHY.
cluster.pgw.unhealthy.sync.started=Cloudera Manager is unreachable, cluster sync started. Please check the instances on Cloud Provider side.
cluster.gateway.change=Starting primary gateway change
cluster.gateway.changed.successfully=Primary gateway successfully changed to {0}
cluster.gateway.change.failed=Primary gateway change failed. Reason: {0}
cluster.ambari.cluster.decommissioning.time=The decommission of the Data nodes will take approximately {0}.

cluster.externaldatabase.deletion.started=External database termination started.
cluster.externaldatabase.deletion.failed=External database termination failed.
cluster.externaldatabase.deletion.finished=External database termination finished.
cluster.externaldatabase.creation.started=External database creation started.
cluster.externaldatabase.creation.failed=External database creation failed.
cluster.externaldatabase.creation.finished=External database creation finished.
cluster.externaldatabase.start.commenced=External database start commenced.
cluster.externaldatabase.start.failed=External database start failed.
cluster.externaldatabase.start.finished=External database start finished.
cluster.externaldatabase.stop.commenced=External database stop commenced.
cluster.externaldatabase.stop.failed=External database stop failed.
cluster.externaldatabase.stop.finished=External database stop finished.
cluster.externaldatabase.stop.not.required=External database stop is not required. Cluter stop finished.

cluster.externaldatabase.upgrade.stop.services=Stopping Runtime Services and Cloudera Manager.
cluster.externaldatabase.upgrade.backup.data=Creating data backup, it might take a while.
cluster.externaldatabase.upgrade.dbserver=Upgrading database server.
cluster.externaldatabase.upgrade.migrate.db.settings=Migrating database settings.
cluster.externaldatabase.upgrade.migrate.services.db.settings=Migrating services' database settings.
cluster.externaldatabase.upgrade.restore.data=Restoring data from the backup, it might take a while.
cluster.externaldatabase.upgrade.start.clustermanager=Restarting Cloudera Manager.
cluster.externaldatabase.upgrade.start.cmservices=Restarting Runtime Services.
cluster.externaldatabase.upgrade.install.pg=Installing Postgres {0} packages if necessary.
cluster.externaldatabase.upgrade.install.pg.failed=Installing Postgres packages failed, please either install them manually or upgrade to an image containing Posgtres {0}, otherwise backup will stop working. Required packages: postgresql{0}-server, postgresql-jdbc, postgresql{0}, postgresql{0}-contrib, postgresql{0}-docs. Error message: {1}.
cluster.externaldatabase.upgrade.started=Upgrading database server to version {0} started.
cluster.externaldatabase.upgrade.finished=Upgrading database server finished.
cluster.externaldatabase.upgrade.failed=Upgrading database server failed with error: {0}
cluster.externaldatabase.upgrade.already.upgraded=Upgrading database server is not needed as it is already on the latest version ({0}).
cluster.externaldatabase.upgrade.not.available=Upgrading database server is not possible as stack is not available, it is in {0} status.
cluster.externaldatabase.upgrade.validation.push.salt.states=Updating the parameters for the orchestration engine.
cluster.externaldatabase.upgrade.validation.backup.validation=Validating if there is enough free space for backup during database server upgrade.
cluster.externaldatabase.upgrade.validation.on.cloudprovider=Validating if the database server upgrade is possible on provider side.
cluster.externaldatabase.upgrade.validation.finished=Database server upgrade validation finished.
cluster.externaldatabase.upgrade.validation.failed=Database server upgrade validation failed. Reason: {0}.
cluster.externaldatabase.upgrade.attached.datahubs.migrate.dbsettings.started=Migration of attached Data Hub clusters' database settings is started.
cluster.externaldatabase.upgrade.attached.datahubs.migrate.dbsettings.finished=Migration of attached Data Hub clusters'' database settings is done: {0}, skipped: {1}. Stopped clusters will be automatically migrated during start.
cluster.externaldatabase.upgrade.attached.datahubs.migrate.dbsettings.failed=Migration of attached Data Hub clusters'' database settings is done: {0}, skipped: {1}, failed: {2}. Stopped clusters will be automatically migrated during start. On failed clusters the migration have to be done manually.

cluster.externaldatabase.certificate.rotation.prerequisites=Cluster prerequisite check for RDS certificate rotation.
cluster.externaldatabase.certificate.rotation.tlssetup=Update Cluster TLS configuration if required.
cluster.externaldatabase.certificate.rotation.getlatest=Obtain latest RDS certificate.
cluster.externaldatabase.certificate.rotation.pushlatest=Pushing latest RDS certificate to the cluster.
cluster.externaldatabase.certificate.rotation.cmrestart=Restarting Cluster Manager service to reload latest RDS certificate.
cluster.externaldatabase.certificate.rotation.rollingservicerestart=Rolling restart of Cluster services to reload latest RDS certificate.
cluster.externaldatabase.certificate.rotation.onprovider=Rotate RDS SSL certificate on cloud provider side to the latest available.
cluster.externaldatabase.certificate.rotation.finished=RDS certificate rotation finished.
cluster.externaldatabase.certificate.rotation.failed=RDS certificate rotation failed.

cluster.prepare.embeddeddatabase.upgrade.inprogress=Preparing embedded database upgrade (space validation on db disk).
cluster.prepare.embeddeddatabase.upgrade.finished=Preparing embedded database upgrade finished.
cluster.prepare.embeddeddatabase.upgrade.failed=Preparing embedded database upgrade failed.

cluster.ccm.upgrade.tunnel.update=Cluster Connectivity Manager tunnel type update.
cluster.ccm.upgrade.push.salt.states=Cluster Connectivity Manager upgrade pushing Salt states.
cluster.ccm.upgrade.reconfigure.nginx=Cluster Connectivity Manager upgrade reconfiguring NGINX.
cluster.ccm.upgrade.register.clusterproxy=Cluster Connectivity Manager upgrade re-register hosts.
cluster.ccm.upgrade.health.check=Cluster Connectivity Manager health check.
cluster.ccm.upgrade.remove.agent=Cluster Connectivity Manager remove previous version's agent.
cluster.ccm.upgrade.remove.agent.failed=Cluster Connectivity Manager remove previous version's agent has been failed.
cluster.ccm.upgrade.deregister.agent=Cluster Connectivity Manager unregister previous version's agent.
cluster.ccm.upgrade.deregister.agent.failed=Cluster Connectivity Manager unregister previous version's agent has been failed.
cluster.ccm.upgrade.failed=Cluster Connectivity Manager upgrade failed.
cluster.ccm.upgrade.finished=Cluster Connectivity Manager upgrade finished.

cluster.set.default.java.version=Setting default Java version to {0}
cluster.set.default.java.version.finished=Default Java version successfully updated
cluster.set.default.java.version.failed=Default Java version update failed: {0}

cluster.kerberosconfig.validation.failed=Kerberos config validation failed with: {0}

cluster.cloudconfig.validation.failed=Cloud config validation failed with the following errors: {0}
cluster.provider.validation.warning=Cloud platform validation succeeded with the following warnings: {0}
cluster.provider.resource.creation.failed=Some resources could not be created by the provider and need to be rolled back: {0}
cluster.provider.resource.rollback.failed=Resource rollback operation has failed. Please check and manually delete provider resources if necessary. Error details:\n{0}

cm.cluster.command.failed=Cloudera Manager command [{0}] failed, you can check the command here: {1}
cm.cluster.command.timeout=Cloudera Manager command [{0}] timed out, you can check the command here: {1}
cm.cluster.command.timeout.parcelactivation=Parcel activation failed, please check the version and status of the following parcel in the Cloudera Manager: {0}
cm.cluster.command.timeout.warning=Cloudera Manager command [{0}] takes unusually long time, you can check the command here: {1}
cm.cluster.services.started=Cloudera Manager services have been started.
cm.cluster.services.starting=Starting Cloudera Manager services.
cm.cluster.services.restarting=Restarting Cloudera Manager services.
cm.cluster.services.rolling.restart=Rolling restart Cloudera Manager services.
cm.cluster.services.stopping=Stopping Cloudera Manager services.
cm.cluster.services.stopped=Cloudera Manager services have been stopped.
cm.cluster.updating.remote.data.context=Cloudera Manager updating remote data context, started.
cm.cluster.updated.remote.data.context=Cloudera Manager updating remote data context, complete.
cm.cluster.securitygroup.too.strict=The security group of the Cloudera Manager node is probably too strict. Please check if all the necessary ports are reachable. Error message: {0}.
cm.cluster.service.deregister.failed=Failed to deregister service(s) from Cloudera Manager. This has to be done manually. It's possible that CM or the instance is not running. 

cluster.bootstrapper.error.nodes.failed=Bootstrap failed on {0} nodes. These nodes will be terminated
cluster.bootstrapper.error.invalide.nodecount=Invalid node count on instance group {0}; Cluster creation failed
cluster.bootstrapper.error.deleting.node=Deleting node: {0}. Decreasing the node count on instance group: {1}

cluster.start.ignored=Cluster start request ignored, cluster is already available
cluster.stop.ignored=Cluster stop request ignored, cluster is already stopped
cluster.host.status.updated=Host [name: {0}] state has been updated to: {1}
cluster.hosts.states.updated=Hosts states has been updated:\n{0}
ambari.cluster.resetting.ambari.database=Resetting Cloudera Manager database
ambari.cluster.ambari.database.reset=Cloudera Manager database has been reset
ambari.cluster.restarting.ambari.server=Restarting Cloudera Manager server
ambari.cluster.restarting.ambari.agent=Restarting Cloudera Manager agents
ambari.cluster.ambari.server.restarted=Cloudera Manager server restarted
ambari.cluster.ambari.agent.restarted=Cloudera Manager agents restarted
cluster.removing.nodes=Removing {0} node(s)
cluster.removing.zombie.nodes=Removing {0} Zombie node(s)
cluster.force.removing.nodes=Removing forcefully {0} node(s)
cluster.removing.node.lost.node.decommission.aborted=Decommission commands for lost nodes {0} were stucked in Cloudera Manager twice, thus were aborted, removing those nodes forcefully.
ambari.cluster.adding.node.to.hostgroup=Adding {0} new host(s) to the host group {1}

cluster.single.master.repair.started=Started repairing Cloudera Manager on recovered master node
cluster.single.master.repair.finished=Finished repairing Cloudera Manager on recovered master node
ambari.cluster.single.master.repair.failed=Repairing Cloudera Manager on recovered master node failed

stack.instance.terminate=Terminating instance {0}
stack.instance.delete=Deleting node {0}. Decreasing the node count on instance group {1}"

recipes.execute=Execute recipes: {0}
cluster.recipes.upload=Upload recipes: {0}

cluster.ambari.cluster.could.not.sync=Cluster can''t be synchronized, status: {0}
cluster.ambari.cluster.synchronized=The cluster state synchronized with Cloudera Manager: {0}
cluster.sync.instance.different.packages=The following packages are installed with different versions on hosts: {0}
cluster.sync.instance.missing.package.versions=There are missing package versions on hosts: {0}
cluster.sync.instance.failedquery.packages=Query of package versions has been failed on hosts: {0}

stack.stop.ignored=Stop request ignored; cluster infrastructure is already stopped.
stack.start.ignored=Start request ignored; cluster infrastructure is already started.

stack.metadata.setup.billing.changed=Billing changed due to upscaling of cluster infrastructure

stack.scaling.host.deleted=Host {0} deleted
stack.scaling.host.delete.failed=Could not delete host {0} from Cloudera Manager
stack.scaling.host.not.found=Host {0} is not found in Cloudera Manager
stack.scaling.billing.changed=Billing changed due to downscaling of cluster infrastructure.
stack.scaling.terminating.host.from.hostgroup=Terminating host: {0} from host group: {1}



stack.sync.instance.status.retrieval.failed=Couldn''t retrieve the status of instance {0} from the cloud provider.
stack.sync.instance.status.couldnt.determine=The state of one or more instances couldn''t be determined: {0}
stack.sync.instance.operation.in.progress=An operation on one or more instances is in progress. Try syncing later.
stack.sync.instance.stopped.on.provider=Some instances were stopped on the cloud provider. Restart or terminate them and try syncing later.
stack.sync.host.deleted=Deleted host {0} from Cloudera Manager as it is marked as terminated by the cloud provider.
stack.sync.instance.removal.failed=Instance {0} is terminated but couldn''t remove host from Cloudera Manager because it still reports the host as healthy. Try syncing later.
stack.sync.host.updated=Host {0} state has been updated to: {1}.
stack.sync.instance.terminated=Instance {0} is marked as terminated by the cloud provider, but couldn''t delete the host from Cloudera Manager.
stack.sync.instance.deleted.cbmetadata=Deleted instance {0} from Cloudbreak metadata because it couldn''t be found on the cloud provider.
stack.sync.instance.deletedbyprovider.cbmetadata=Deleted instance {0} from Cloudbreak metadata because it was deleted by the cloud provider.
stack.sync.versions.from.cm.to.db.succeeded=Reading CM and active parcel versions from CM server succeeded: {0}
stack.sync.versions.from.cm.to.db.failed=Reading CM and active parcel versions from CM server has failure(s): {0}
stack.sync.versions.from.cm.to.db.missing.versions=Please check the following node(s) as they could not report CM version: {0}
stack.sync.instance.updated=Updated metadata of instance {0} to {1} as the cloud provider reported it as {1}.
stack.sync.instance.failed=Updated metadata of instance {0} to failed, because update was in progress, but instance isn''t member of the cluster.

datalake.upgrade=Upgrade process initiated for runtime {0}, the target image id is: {1}.
datalake.rolling.upgrade=Rolling upgrade process initiated for runtime {0}, the target image id is: {1}.
datalake.upgrade.preparation=Upgrade preparation process initiated for runtime {0}, target image id is {1}.
datalake.upgrade.could.not.start=Upgrade process could not be started due to the following reason: {0}
datalake.ccm.upgrade=Cluster Connectivity Manager upgrade initiated.
datalake.ccm.upgrade.error.environment.notlatest=Invalid state: environment {0} is not upgraded to the latest Cluster Connectivity Manager yet.
datalake.ccm.upgrade.error.invalid.count=More than one Data Lake is found for environment {0}. This case is not handled yet.
datalake.ccm.upgrade.no.datalake=Environment {0} has no Data Lake present.
datalake.ccm.upgrade.already.upgraded=Data Lake is already at the latest Cluster Connectivity Manager version.
datalake.ccm.upgrade.not.upgradeable=Data Lake is not upgradeable.
datalake.ccm.upgrade.not.available=Data Lake is not in Available state for upgrade.
datalake.ccm.upgrade.failed=Data Lake upgrade to the latest Cluster Connectivity Manager version failed.
datalake.ccm.upgrade.inprogress=Data Lake upgrade to the latest Cluster Connectivity Manager is in progress.
datalake.db.certificate.rotation.not.available=Data Lake is not in Available state for certificate rotation.
datalake.db.certificate.rotation=Certificate rotation initiated.
datalake.db.certificate.rotation.no.datalake=Environment {0} has no Data Lake present.
datalake.db.certificate.rotation.failed=Data Lake Database upgrade to the latest certificate version failed.
datalake.db.certificate.rotation.inprogress=Data Lake Database rotation is in progress.
datalake.db.ssl.migration.not.available=Data Lake is not in Available state for migration to ssl.
datalake.db.ssl.migration=Migration to ssl initiated.
datalake.db.ssl.migration.no.datalake=Environment {0} has no Data Lake present.
datalake.db.ssl.migration.failed=Data Lake Database migration to ssl failed.
datalake.db.ssl.migration.inprogress=Data Lake Database migration is in progress.
datalake.proxy.modification.inprogress=Data Lake proxy config modification in progress
datalake.proxy.modification.failed=Data Lake proxy config modification failed
datalake.proxy.modification.finished=Data Lake proxy config modification finished
datalake.database.server.upgrade.requested=Data Lake database server upgrade was requested to version {0}.
datalake.database.server.upgrade.failed=Data Lake database server upgrade failed. Reason: {0}.
datalake.database.server.upgrade.inprogress=Data Lake database server upgrade is in progress.
datalake.database.server.upgrade.succeeded=Data Lake database server upgrade succeeded.

datahub.ccm.upgrade.error.environment.notlatest=Invalid state: environment {0} is not upgraded to the latest Cluster Connectivity Manager yet.
datahub.ccm.upgrade.not.available=Data Hub is not in Available state for upgrade.
datahub.ccm.upgrade.already.upgraded=Data Hub is already at the latest Cluster Connectivity Manager version.
datahub.ccm.upgrade.not.upgradeable=Data Lake is not upgradeable.

cluster.proxy.modification.saltstate=Updating orchestrator state with modified proxy config settings
cluster.proxy.modification.cm=Updating Cloudera Manager with modified proxy config settings
cluster.proxy.modification.success=Proxy config modification was successful
cluster.proxy.modification.failed=Proxy config modification failed: {0}
cluster.manager.upgrade=Upgrading Cluster Manager.
cluster.manager.upgrade.not.needed=Upgrading Cluster Manager is not needed as its buildnumber is the same: {0}.
cluster.manager.upgrade.failed=Failed to upgrade Cluster Manager. Reason: {0}.
cluster.manager.upgrade.finished=Cluster Manager upgrade was successful. New version is {0}.
cluster.upgrade=Upgrading Cluster Runtime.
cluster.upgrade.not.needed=Upgrading Cluster Runtime is not needed. The required parcels are already present on the cluster.
cluster.upgrade.failed=Failed to upgrade Cluster Runtime. Reason: {0}.
cluster.upgrade.finished.newversion=Cluster Runtime upgrade was successful. New version is {0}.
cluster.upgrade.finished.noversion=Cluster upgrade was successful.
cluster.upgrade.preparation.started=Cluster upgrade preparation started for runtime {0} based on image: {1}.
cluster.upgrade.preparation.finished=Cluster upgrade preparation finished.
cluster.upgrade.preparation.failed=Cluster upgrade preparation failed: {0}
cluster.upgrade.download.parcel=Downloading the new parcel
cluster.upgrade.distribute.parcel=Distributing the new parcel
cluster.upgrade.activate.parcel=Activating the new parcel
cluster.upgrade.start.upgrade=Calling upgrade on Runtime
cluster.upgrade.start.rolling.upgrade=Calling rolling upgrade on Runtime
cluster.upgrade.start.post-upgrade=Calling post upgrade on Runtime
cluster.upgrade.validation.started=Cluster upgrade validation started
cluster.upgrade.validation.finished=Cluster upgrade validation finished
cluster.upgrade.validation.failed=Cluster upgrade validation failed: {0}
cluster.upgrade.validation.skipped=Cluster upgrade validation skipped. Reason: {0}

cluster.salt.update.started=Salt update has been started
cluster.salt.update.failed=Failed to update salt states
cluster.salt.update.finished=Salt successfully updated

cluster.pillar.config.update.started=Update of cluster configuration started.
cluster.pillar.config.update.finished=Cluster configuration successfully updated.
cluster.pillar.config.update.failed=Failed to update cluster configuration,

cluster.salt.passwordrotate.started=SaltStack user password rotation started
cluster.salt.passwordrotate.started.fallback=SaltStack user password rotation started with fallback implementation, please upgrade your cluster for a better user experience
cluster.salt.passwordrotate.finished=SaltStack user password successfully rotated, restored previous status
cluster.salt.passwordrotate.failed=SaltStack user password rotation failed with error: {0}

resource.blueprint.created=Blueprint created.
resource.blueprint.deleted=Blueprint deleted.
resource.sdx.created=Datalake cluster created.
resource.sdx.is.running=Datalake cluster is running.
resource.sdx.deleted=Datalake cluster deleted.
resource.sdx.deletionstarted=Started deletion of datalake {0}.
resource.sdx.deletionfinished=Finished deletion of datalake {0}.
resource.sdx.deletionfailed=Failed to delete datalake {0}.
resource.sdx.deletedonproviderside=Datalake cluster deleted on provider side.
resource.sdx.stack_deletion_in_progress=The deletion of datalake {0}'s stack is in progress. 
resource.sdx.envwait=Datalake cluster is waiting for the environment.
resource.sdx.envfinished=Environment for datalake has been created.
resource.sdx.repair.started=Datalake repair flow started.
resource.sdx.repair.finished=Datalake repair flow finished.
resource.sdx.repair.failed=Datalake repair flow failed.
resource.sdx.failed=Datalake cluster creation failed.
resource.sdx.rdsdeletionstarted=Datalake RDS deletion started
resource.sdx.rdsdeletionfailed=Datalake RDS deletion failed
resource.sdx.requested=Datalake requested.
resource.sdx.provisionstarted=Datalake cluster provision started.
resource.sdx.stack_provision_in_progress=Datalake cluster stack provision in progress.
resource.sdx.stack_provision_finished=Datalake cluster stack provision finished.
resource.sdx.provisionfinished=Datalake cluster provision finished.
resource.sdx.rdsdeletionfinished=Datalake RDS deletion finished.
resource.sdx.rdscreationstarted=Datalake RDS creation started.
resource.sdx.rdscreationfailed=Datalake RDS creation failed.
resource.sdx.rdscreationfinished=Datalake RDS creation finished.
resource.sdx.rdsstartstarted=Datalake RDS start operation started.
resource.sdx.rdsstartfailed=Datalake RDS start operation failed.
resource.sdx.rdsstartfinished=Datalake RDS start operation finished.
resource.sdx.rdsstopstarted=Datalake RDS stop operation started.
resource.sdx.rdsstopfailed=Datalake RDS stop operation failed.
resource.sdx.rdsstopfinished=Datalake RDS stop operation finished.
resource.sdx.start.started=Datalake start flow executed.
resource.sdx.start.finished=Datalake start flow finished.
resource.sdx.start.failed=Datalake start failed.
resource.sdx.stop.started=Datalake stop executed.
resource.sdx.stop.finished=Datalake stop finished.
resource.sdx.stop.failed=Datalake stop failed.
resource.sdx.detach.started=Datalake detach started. Original datalake renamed to {0}.
resource.sdx.detach.finished=Datalake detach finished.
resource.sdx.detach.deletion.failed=Deletion of detached datalake {0} failed. Please try to manually delete the datalake using the CDP CLI (using force flag if necessary).
resource.sdx.update_lb_dns.finished=Update of load balancer DNS for datalake finished.
resource.sdx.update_lb_dns.failed=Update of load balancer DNS for datalake failed. Reason: {0}.
resource.sdx.validation.skipped=Datalake object storage validation skipped. Reason: {0}.
resource.sdx.validation.failed=Datalake object storage validation failed. Reason: {0}.
resource.sdx.change.image.started=Datalake change image flow started.
resource.sdx.datalake.os.upgrade.started=Datalake OS upgrade flow started.
resource.sdx.datalake.rolling.os.upgrade.started=Datalake rolling OS upgrade flow started.
resource.sdx.datalake.upgrade.started=Datalake upgrade flow started.
resource.sdx.datalake.rolling.upgrade.started=Datalake rolling upgrade flow started.
resource.sdx.datalake.upgrade.preparation.started=Datalake upgrade preparation flow started.
resource.sdx.datalake.upgrade.finished=Datalake upgrade flow finished.
resource.sdx.datalake.upgrade.preparation.finished=Datalake upgrade preparation flow finished.
resource.sdx.datalake.upgrade.failed=Datalake upgrade flow failed.
resource.sdx.datalake.upgrade.preparation.failed=Datalake upgrade preparation flow failed.
resource.sdx.sync.failed=Datalake sync failed
resource.workspace.created=Workspace created. {0}
resource.workspace.deleted=Workspace deleted. {0}
resource.clustertemplate.created=Cluster template created.
resource.clustertemplate.deleted=Cluster template deleted.
resource.credential.created=Credential created.
resource.credential.deleted=Credential deleted.
resource.credential.modified=Credential modified.
resource.ldap.created=Ldap created.
resource.ldap.deleted=Ldap deleted.
resource.mpack.created=Management pack created.
resource.mpack.deleted=Management pack deleted.
resource.kubernetesconfig.created=Kubernetes config created.
resource.kubernetesconfig.modified=Kubernetes config modified.
resource.kubernetesconfig.deleted=Kubernetes config deleted.
resource.network.created=Network created.
resource.network.deleted=Network deleted.
resource.recipe.created=Recipe created.
resource.recipe.deleted=Recipe deleted.
resource.rdsconfig.created=Rds config created.
resource.rdsconfig.deleted=Rds config deleted.
resource.proxyconfig.created=Proxy config created.
resource.proxyconfig.deleted=Proxy config deleted.
resource.securitygroup.created=Security group created.
resource.securitygroup.deleted=Security group deleted.
resource.template.created=Template created.
resource.template.deleted=Template deleted.
resource.topology.created=Topology created.
resource.topology.deleted=Topology deleted.
resource.imagecatalog.created=Image catalog created.
resource.imagecatalog.deleted=Image catalog deleted.
resource.maintenancemode.enabled=Maintenance mode enabled.
resource.maintenancemode.disabled=Maintenance mode disabled.

stack.image.update.started=Changing stack image
stack.image.update.finished=Stack image change finished
stack.image.update.failed=Image change failed with message: {0}
stack.image.update.packages.different=Missing packages from image: [{0}] Different package versions on image: [{1}] 
stack.image.update.cloudplatform.different=New image platform(s) [{0}] and current Stack platform [{1}] is different.
stack.image.update.osversion.different=New image OS [{0}] and OS type [{1}] is different from current OS [{2}] and OS type [{3}]

maintenance.mode.validation.started=The validation of the repo and image settings has begun
maintenance.mode.validation.finished.warn=Validation finished with warnings: {0}
maintenance.mode.validation.finished.nowarn=Validation finished successfully.
maintenance.mode.validation.failed=Validation failed. Reason: {0}

environment.initialization.started=Environment initialization started
environment.initialization.failed=Environment initialization failed
environment.validation.started=Environment creation request validation started
environment.validation.failed=Environment creation request validation failed
environment.validation.warnings=Environment creation validation identified warning(s): {0}.
environment.validation.skipped=Environment creation request validation skipped. Reason: {0}.
environment.compute.cluster.creation.started=Compute cluster creation started
environment.compute.cluster.creation.finished=Compute cluster creation finished
environment.compute.cluster.creation.failed=Compute cluster creation failed. Reason: {0}
environment.compute.cluster.reinitialization.started=Compute cluster reinitialization started
environment.compute.cluster.reinitialization.finished=Compute cluster reinitialization finished
environment.compute.cluster.reinitialization.failed=Compute cluster reinitialization failed. Reason: {0}
environment.compute.cluster.waiting.started=Waiting for compute cluster creation
environment.compute.cluster.waiting.failed=Waiting for compute cluster creation failed
environment.network.creation.started=Network creation/registration started for environment
environment.network.creation.failed=Network creation/registration failed for environment
environment.publickey.creation.started=Public Key creation/registration started for environment
environment.publickey.creation.failed=Public Key creation/registration failed for environment
environment.resource.encryption.initialization.started=Resource Encryption initialization started for environment
environment.resource.encryption.initialization.failed=Resource Encryption initialization failed for environment
environment.freeipa.creation.started=FreeIPA creation/registration started for environment
environment.freeipa.creation.failed=FreeIPA creation/registration failed for environment
environment.creation.finished=Environment creation successfully finished
environment.creation.failed=Environment creation failed

environment.ssh.deletion.skipped=SSH key pair deletion skipped. The following key pair needs manual cleanup: {0}.
environment.ssh.deletion.applied=SSH key pair deletion applied. Deleting the following key pair: {0}.
environment.network.deletion.started=Network deletion/deregistration started
environment.publickey.deletion.started=Public Key deletion/deregistration started
environment.clusterdefinition.deletion.started=Cluster definition deletion started
environment.database.deletion.started=Database deletion/deregistration started
environment.freeipa.deletion.started=FreeIPA deletion/deregistration started
environment.xp.deletion.started=Experience deletion started
environment.compute.clusters.deletion.started=Compute cluster deletion started
environment.idbroker.mappings.deletion.started=IDBroker mappings deletion/deregistration started
environment.s3guard.table.deletion.started=S3Guard table deletion/deregistration started
environment.ums.resource.deletion.started=Environment assigned UMS resources deletion started
environment.structured.event.cleanup.started=Environment related event cleanup started
environment.datahubs.deletion.started=Data Hub clusters deletion/deregistration started
environment.datalakes.deletion.started=Data Lake clusters deletion/deregistration started
environment.resource.encryption.deletion.started=Resource Encryption deletion started
environment.deletion.finished=Environment deletion successfully finished
environment.deletion.failed=Environment deletion failed
environment.deletion.failed.with.reason=Environment deletion failed. Reason: {0}

environment.stop.datahub.started=Datahub stopped for Environment
environment.stop.datahub.failed=Datahub stop failed for Environment
environment.stop.datalake.started=Datalake stopped for Environment
environment.stop.datalake.failed=Datalake stop failed for Environment
environment.stop.freeipa.started=Freeipa stopped for Environment
environment.stop.freeipa.failed=Freeipa stop failed for Environment

environment.stop.success=Environment successfully stopped
environment.stop.failed=Environment failed to stop

environment.start.datahub.started=Datahub started for Environment
environment.start.datahub.failed=Datahub start failed for Environment
environment.restart.datahub.started=Datahubs restart started for Environment
environment.restart.datahub.failed=Datahubs restart failed for Environment
environment.restart.datahub.finished=Datahubs restart finished for Environment
environment.start.datalake.started=Datalake started for Environment
environment.start.datalake.failed=Datalake start failed for Environment
environment.start.freeipa.started=Freeipa started for Environment
environment.start.freeipa.failed=Freeipa start failed for Environment
environment.start.syncusers.started=Synchronize users started for Environment
environment.start.syncusers.failed=Synchronize users failed for Environment

environment.sync.finished=Environment sync is finished and new status is found, the new status is {0}
environment.salt.passwordrotate.finished=Environment SaltStack user password rotation finished
environment.salt.passwordrotate.failed=Environment SaltStack user password rotation failed. {0}

environment.start.success=Environment successfully started
environment.start.failed=Environment failed to start

environment.loadbalancer.update.env.started=Environment updated for load balancer
environment.loadbalancer.update.env.failed=Load balancer update for environment failed
environment.loadbalancer.update.stack.started=Stacks updated for load balancer
environment.loadbalancer.update.stack.failed=Load balancer update for stacks failed
environment.loadbalancer.update.finished=Load balancer update finished
environment.loadbalancer.update.success=Load balancer update successful
environment.loadbalancer.update.failed=Load balancer update failed

environment.upgrade.ccm.validation.started=Validation for CCM upgrade started
environment.upgrade.ccm.validation.failed=Validation for CCM upgrade failed
environment.upgrade.ccm.freeipa.started=Upgrading CCM on FreeIPA started
environment.upgrade.ccm.freeipa.failed=Upgrading CCM on FreeIPA failed
environment.upgrade.ccm.tunnelupdate.started=Updating tunnel type started
environment.upgrade.ccm.tunnelupdate.failed=Updating tunnel type failed
environment.upgrade.ccm.datalake.started=Upgrading CCM on Data Lake started
environment.upgrade.ccm.datalake.failed=Upgrading CCM on Data Lake failed
environment.upgrade.ccm.datahub.started=Upgrading CCM on Data Hub clusters started
environment.upgrade.ccm.datahub.failed=Upgrading CCM on Data Hub clusters failed
environment.upgrade.ccm.rollback=Rolling back CCM upgrade on last failed component
environment.upgrade.ccm.finished=Upgrading CCM finished
environment.upgrade.ccm.finished.witherrors=Upgrading CCM finished. {0}
environment.upgrade.ccm.failed=Upgrading CCM failed

environment.vertical.scale.validation.started=Validation for vertical scaling FreeIPA started
environment.vertical.scale.validation.failed=Validation for vertical scaling FreeIPA failed with error {}
environment.vertical.scale.freeipa.started=Vertical scaling FreeIPA started on group {0} from instance type {1} to {2}
environment.vertical.scale.freeipa.failed=Vertical scaling FreeIPA failed on group {0} from instance type {1} to {2}: {3}
environment.vertical.scale.finished=Vertical scaling FreeIPA finished on group {0} from instance type {1} to {2}
environment.vertical.scale.failed=Vertical scaling FreeIPA to instance type {0} was not successful, please retry!

environment.proxy.modification.started=Proxy config modification to {0} started for Environment
environment.proxy.modification.freeipa.started=Proxy config modification started for FreeIPA instances
environment.proxy.modification.datalake.started=Proxy config modification started for DataLake instances
environment.proxy.modification.datahubs.started=Proxy config modification started for all attached DataHubs
environment.proxy.modification.failed=Proxy config modification to {0} failed for Environment. Details: {1}
environment.proxy.modification.finished=Proxy config modification finished for Environment

credential.azure.interactive.created=Azure interactive credential app successfully created
credential.azure.interactive.failed=Azure interactive credential app creation failed
credential.azure.interactive.status=Azure interactive credential status

retry.flow.start=Retrying {0} operation from the failed step.
common.bad.request.notification.pattern=Error during operation: {0}

datalake.database.backup=Datalake database backup initiated.
datalake.database.backup.finished=Database backup was successful.
datalake.database.backup.failed=Failed to backup database. Reason: {0}.
datalake.database.backup.validation.failed=Backup database validation failed. Reason: {0}.
datalake.database.backup.could.not.start=Database backup could not be started due to the following reason: {0}
datalake.database.restore=Datalake database restore initiated.
datalake.database.restore.finished=Datalake database restore is successful.
datalake.database.restore.failed=Failed to restore database. Reason: {0}.
datalake.database.restore.validation.failed=Restore database validation failed. Reason: {0}.
datalake.database.restore.could.not.start=Database restore could not be started due to the following reason: {0}
datalake.backup.in.progress=Datalake backup in progress.
datalake.backup.validation.in.progress=Datalake backup validation in progress.
datalake.restore.in.progress=Datalake restore in progress.
datalake.restore.validation.in.progress=Datalake restore validation in progress.
datalake.backup.finished=Datalake backup is complete.
datalake.backup.cancelled=Datalake backup was cancelled.
datalake.backup.validation.finished=Datalake backup validation complete.
datalake.restore.finished=Datalake restore is complete.
datalake.restore.validation.finished=Datalake restore validation is complete.
datalake.backup.failed=Datalake backup failed. Reason: {0}
datalake.backup.validation.failed=Datalake backup validation failed. Reason: {0}
datalake.restore.failed=Datalake restore failed. Reason: {0}
datalake.restore.validation.failed=Datalake restore validation failed. Reason: {0}
datalake.cert.renewal.started=Datalake certificate renewal started.
datalake.cert.renewal.failed=Datalake certificate renewal failed. Reason: {0}
datalake.cert.renewal.finished=Datalake certificate renewal finished.

datalake.recovery.requested=Datalake upgrade recovery requested. Cluster will be terminated and re-launched with the original runtime.
datalake.recovery.finished=Datalake recovery is complete.
datalake.recovery.in.progress=Datalake recovery is in progress.
datalake.recovery.failed=Datalake recovery failed.
datalake.recovery.bringup.failed=Datalake stack recovery failed.
datalake.recovery.bringup.finished=Datalake stack successfully recovered, creating datalake cluster.
datalake.recovery.teardown.finished=Datalake stack tear-down for recovery completed successfully.
datalake.recovery.started=Datalake recovery started.
datalake.resize.triggered=Datalake resize initiated.
datalake.resize.complete=Datalake resize complete.
datalake.resize.failure.restore=Datalake resize failed during restore. Please attempt the restore again manually and then manually delete the original detached datalake. You can find its name in the event history.
datalake.resize.failure.create=Datalake resize failed during creation of the new Datalake. Please attempt to run the automatic resize recovery process for the Datalake by following our documentation. If there are any issues running the recovery process, please reach out to our support team.

datalake.datahub.refresh.in.progress=Datahub refresh in progress
datalake.datahub.refresh.failed=Datahub refresh failed. Reason: {0}

datalake.salt.passwordrotation.in.progress=Datalake SaltStack user password rotation in progress
datalake.salt.passwordrotation.failed=Datalake SaltStack user password rotation failed. Reason: {0}
datalake.salt.passwordrotation.finished=Datalake SaltStack user password rotation finished

datalake.salt.update.in.progress=Datalake SaltStack update in progress
datalake.salt.update.failed=Datalake SaltStack update failed. Reason: {0}
datalake.salt.update.finished=Datalake SaltStack update finished

environment.stack.config.updates.started=Environment configuration update of clusters started.
environment.stack.config.updates.finished=Environment configuration update of clusters finished.
environment.stack.config.updates.failed=Environment configuration update of clusters failed due to the following reason: {0}  

resource.sdx.cert.rotation.started=Datalake certificate rotation started.
resource.sdx.cert.rotation.failed=Datalake certificate rotation failed. Reason: {0}
resource.sdx.cert.rotation.finished=Datalake certificate rotation finished.

stack.lb.update.create.entity=Creating load balancer entities.
stack.lb.update.create.cloud.resource=Creating load balancer cloud resources.
stack.lb.update.collect.metadata=Collecting load balancer metadata.
stack.lb.update.register.public.dns=Registering load balancer public DNS.
stack.lb.update.register.public.dns.failed=Load balancer public DNS registration failed. It's possible that instances are not reachable, if that is the case, please contact the support, so they can register this DNS entry manually through the PEM service.
stack.lb.update.register.freeipa.dns=Register load balancer FreeIPA DNS.
stack.lb.update.update.cm.config=Updating CM frontend URL.
stack.lb.update.restart.cm=Restarting the CM server.
stack.lb.update.finished=Load balancer creation is finished.
stack.lb.update.failed=Load balancer creation failed. Reason: {0}

stack.dynamic.entitlement.started=Telemetry update started. This process usually takes 3-4 minutes, during which the cluster operations (such as repair, scaling, and upgrade) do not work.
stack.dynamic.entitlement.finished=Telemetry update completed.
stack.dynamic.entitlement.failed=Telemetry update did not complete. Reason: {0}

aws.variant.migration.successful=Successfully migrated the cloud formation template. The CF template and autoscaling group were removed
aws.variant.migration.failed=Failed to migrate the cloud formation template

datalake.default.java.version.change.in.progress=Changing the default Java version to {0} in Data Lake is in progress
datalake.default.java.version.change.failed=Failed to change the default Java version in Data Lake: {0}

datalake.verticalscale.validation.in.progress=Validation for vertical scale Data Lake is in progress
datalake.verticalscale.validation.failed=Validation for vertical scale Data Lake failed {0}
datalake.verticalscale.datalake.in.progress=Vertical scale Data Lake is in progress
datalake.verticalscale.datalake.failed=Vertical scale Data Lake failed
datalake.verticalscale.failed=Vertical scale failed on Data Lake {0} please use a different instance type or retry the operation

secret.rotation.in.progress=Secret rotation started
secret.rotation.finished=Secret rotation finished
secret.rotation.failed=Secret rotation failed
secret.rotation.rollback.in.progress=Secret rotation rollback started
secret.rotation.rollback.finished=Secret rotation rollback finished
secret.rotation.rollback.failed=Secret rotation rollback failed
secret.rotation.finalize.in.progress=Secret rotation finalize started
secret.rotation.finalize.finished=Secret rotation finalize finished
secret.rotation.finalize.failed=Secret rotation finalize failed
secret.rotation.step={0}

RotationFlowExecutionType.PREVALIDATE=Prevalidate
RotationFlowExecutionType.ROTATE=Rotate
RotationFlowExecutionType.ROLLBACK=Rolling back
RotationFlowExecutionType.FINALIZE=Finalize

CloudbreakSecretType.DATALAKE_EXTERNAL_DATABASE_ROOT_PASSWORD=Database root password
CloudbreakSecretType.INTERNAL_DATALAKE_CM_SERVICE_SHARED_DB=CM services' shared database user and password for Data Lake
CloudbreakSecretType.INTERNAL_DATALAKE_DEMO_SECRET=Data Lake demo secret
CloudbreakSecretType.DATAHUB_DEMO_SECRET=Data Hub demo secret
CloudbreakSecretType.CLUSTER_CB_CM_ADMIN_PASSWORD=Cloudera Manager admin user and password
CloudbreakSecretType.CLUSTER_MGMT_CM_ADMIN_PASSWORD=Cloudera Manager management admin user and password
CloudbreakSecretType.DATAHUB_EXTERNAL_DATABASE_ROOT_PASSWORD=Database root password
CloudbreakSecretType.CLUSTER_CM_DB_PASSWORD=Password is used for Cloudera Manager Database management by the Public Cloud Control Plane. It has admin priviliges over CM database and mostly used during provision and upgrades operations.
CloudbreakSecretType.USER_KEYPAIR=User SSH key pair
CloudbreakSecretType.IDBROKER_CERT=IDBroker certificate
CloudbreakSecretType.GATEWAY_CERT=Gateway certificate
CloudbreakSecretType.CLUSTER_CM_SERVICES_DB_PASSWORD=Database user and password of Cloudera Manager services
CloudbreakSecretType.SALT_BOOT_SECRETS=Salt boot secrets
CloudbreakSecretType.SALT_MASTER_KEY_PAIR=Salt master key pair
CloudbreakSecretType.DATAHUB_CM_SERVICE_SHARED_DB=CM services' shared database user and password for Data Hub
CloudbreakSecretType.INTERNAL_DATALAKE_CM_INTERMEDIATE_CA_CERT=Data Lake intermediate CA certificate renewal
CloudbreakSecretType.DATAHUB_CM_INTERMEDIATE_CA_CERT=Data Hub intermediate CA certificate renewal
CloudbreakSecretType.CLUSTER_LDAP_BIND_PASSWORD=Bind password renewal for FreeIPA LDAP
CloudbreakSecretType.DATAHUB_SSSD_IPA_PASSWORD=SSSD/ipa client password rotation for Data Hub
CloudbreakSecretType.INTERNAL_DATALAKE_SSSD_IPA_PASSWORD=SSSD/ipa client password rotation for Data Lake
CloudbreakSecretType.DATAHUB_DBUS_UMS_ACCESS_KEY=Dbus related machine user's access key credentials' rotation
CloudbreakSecretType.STACK_ENCRYPTION_KEYS=Encryption keys used for encrypting the LUKS volume passphrase and with the userdata secret resources
CloudbreakSecretType.LUKS_VOLUME_PASSPHRASE=Passphrase protecting the key used to encrypt and decrypt the LUKS volume

DatalakeSecretType.DATALAKE_CB_CM_ADMIN_PASSWORD=Cloudera Manager admin user and password
DatalakeSecretType.DATALAKE_MGMT_CM_ADMIN_PASSWORD=Cloudera Manager management admin user and password
DatalakeSecretType.DATALAKE_DATABASE_ROOT_PASSWORD=Password is created by the Cloud Provider and provides administrative access over the RDBMS attached to your cluster.
DatalakeSecretType.DATALAKE_CM_DB_PASSWORD=Password is used for Cloudera Manager Database management by the Public Cloud Control Plane. It has admin priviliges over CM database and mostly used during provision and upgrades operations.
DatalakeSecretType.DATALAKE_USER_KEYPAIR=User SSH key pair
DatalakeSecretType.DATALAKE_IDBROKER_CERT=IDBroker certificate
DatalakeSecretType.DATALAKE_GATEWAY_CERT=Gateway certificate
DatalakeSecretType.DATALAKE_CM_SERVICE_DB_PASSWORD=Database user and password of Cloudera Manager services
DatalakeSecretType.DATALAKE_CM_SERVICE_SHARED_DB=CM services' shared database user and password for Data Lake
DatalakeSecretType.DATALAKE_SALT_BOOT_SECRETS=Salt boot secrets
DatalakeSecretType.DATALAKE_SALT_MASTER_KEY_PAIR=Salt master key pair
DatalakeSecretType.DATALAKE_DEMO_SECRET=Data Lake demo secret
DatalakeSecretType.DATALAKE_CM_INTERMEDIATE_CA_CERT=Data Lake intermediate CA certificate renewal
DatalakeSecretType.DATALAKE_LDAP_BIND_PASSWORD=Data Lake bind password renewal for FreeIPA LDAP
DatalakeSecretType.DATALAKE_SSSD_IPA_PASSWORD=SSSD/ipa client password rotation for Data Lake
DatalakeSecretType.STACK_ENCRYPTION_KEYS=Encryption keys used for encrypting the LUKS volume passphrase and with the userdata secret resources
DatalakeSecretType.LUKS_VOLUME_PASSPHRASE=Passphrase protecting the key used to encrypt and decrypt the LUKS volume

FreeIpaSecretType.FREEIPA_ADMIN_PASSWORD=Password is utilized by the FreeIPA Management System to perform administrative operations during cluster creation, user syncronisation, scaling and upgrades.
FreeIpaSecretType.FREEIPA_LDAP_BIND_PASSWORD=FreeIPA LDAP bind password
FreeIpaSecretType.FREEIPA_SALT_BOOT_SECRETS=Salt boot secrets
FreeIpaSecretType.CCMV2_JUMPGATE_AGENT_ACCESS_KEY=CCM V2 Jumpgate agent access key
FreeIpaSecretType.FREEIPA_DEMO_SECRET=FreeIPA Demo secret
FreeIpaSecretType.FREEIPA_KERBEROS_BIND_USER=FreeIPA kerberos bind user
FreeIpaSecretType.FREEIPA_USERSYNC_USER_PASSWORD=FreeIPA user sync related user's password
FreeIpaSecretType.FREEIPA_STACK_ENCRYPTION_KEYS=Encryption keys used for encrypting the LUKS volume passphrase and with the userdata secret resources
FreeIpaSecretType.FREEIPA_LUKS_VOLUME_PASSPHRASE=Passphrase protecting the key used to encrypt and decrypt the LUKS volume
FreeIpaSecretType.USER_KEYPAIR=User SSH key pair

RedbeamsSecretType.REDBEAMS_EXTERNAL_DATABASE_ROOT_PASSWORD=Database root password

CloudbreakSecretRotationStep.CM_USER=Changing Cloudera Manager user.
CloudbreakSecretRotationStep.SALT_PILLAR=Updating salt pillars.
CloudbreakSecretRotationStep.SALT_STATE_APPLY=Applying salt states.
CloudbreakSecretRotationStep.SALT_STATE_RUN=Running salt states.
CloudbreakSecretRotationStep.CLUSTER_PROXY_REREGISTER=Reregistering cluster proxy configuation.
CloudbreakSecretRotationStep.CLUSTER_PROXY_UPDATE=Updating cluster proxy configuration.
CloudbreakSecretRotationStep.CM_SERVICE_ROLE_RESTART=Restarting Cloudera Manager service role.
CloudbreakSecretRotationStep.CM_SERVICE=Updating Cloudera Manager service configuration.
CloudbreakSecretRotationStep.UMS_DATABUS_CREDENTIAL=Updating Dbus machine user's access key in user management services.

CommonSecretRotationStep.VAULT=Updating secret values.
CommonSecretRotationStep.CUSTOM_JOB=
CommonSecretRotationStep.REDBEAMS_ROTATE_POLLING=Waiting for database management service.
CommonSecretRotationStep.FREEIPA_ROTATE_POLLING=Waiting for FreeIPA service.
CommonSecretRotationStep.CLOUDBREAK_ROTATE_POLLING=Waiting for core service.
CommonSecretRotationStep.SALTBOOT_CONFIG=Updating system service configuration on the gateway node.
CommonSecretRotationStep.USER_DATA=Updating secrets in userdata templates on provider side.

RedbeamsSecretRotationStep.PROVIDER_DATABASE_ROOT_PASSWORD=Changing root password on provider side.

FreeIpaSecretRotationStep.FREEIPA_ADMIN_USER_PASSWORD=FreeIPA admin user password
FreeIpaSecretRotationStep.FREEIPA_DIRECTORY_MANAGER_PASSWORD=FreeIPA directory manager password
FreeIpaSecretRotationStep.SALT_PILLAR_UPDATE=FreeIPA salt pillar update
FreeIpaSecretRotationStep.CCMV2_JUMPGATE=CCM V2 Jumpgate agent access key
FreeIpaSecretRotationStep.LAUNCH_TEMPLATE=FreeIPA launch template
FreeIpaSecretRotationStep.SALT_STATE_APPLY=FreeIPA salt state apply
FreeIpaSecretRotationStep.FREEIPA_USER_PASSWORD=Password change of related user in ipa server.
FreeIpaSecretRotationStep.SALT_STATE_RUN=Running salt states.

instance.metadata.update.started=Metadata update for instances on provider side has been started, {0}.
instance.metadata.update.finished=Metadata update for instances on provider side has been finished.
instance.metadata.update.failed=Metadata update for instances on provider side failed, reason: {0}
datalake.instance.metadata.update.started=Metadata update for instances for Data Lake has been started.
datalake.instance.metadata.update.finished=Metadata update for instances for Data Lake has been finished.
datalake.instance.metadata.update.failed=Metadata update for instances for Data Lake failed.
InstanceMetadataUpdateType.IMDS_HTTP_TOKEN_REQUIRED=IMDSv2 (HTTP tokens) will be required
InstanceMetadataUpdateType.IMDS_HTTP_TOKEN_OPTIONAL=IMDSv2 (HTTP tokens) will be optional

datalake.horizontalscale.validation.in.progress=Validation for horizontal scale Data Lake is in progress
datalake.horizontalscale.validation.failed=Validation for Data Lake horizontal scaling failed. Issues: {0}
datalake.horizontalscale.in.progress=Data Lake horizontal scaling is in progress
datalake.horizontalscale.services.restart.in.progress=Data Lake horizontal scaling rolling restart is in progress
datalake.horizontalscale.finished=Data Lake horizontal scaling finished
datalake.horizontalscale.failed=Data Lake horizontal scaling failed with {0}

datalake.disk.update.validation.in.progress=Validation for disk update on Data Lake is in progress.
datalake.disk.update.validation.failed=Validation for disk update on Data Lake failed.
datalake.disk.update.datalake.in.progress=Disk update on Data Lake is in progress.
datalake.disk.update.failed=Disk update failed on Data Lake please validate the input or retry the operation.
datalake.image.validation.warning=Data Lake image validation succeeded with the following warnings: {0}

datahub.disk.update.validation.in.progress=Validation for disk update on Data Hub is in progress.
datahub.disk.update.validation.failed=Validation for disk update on Data Hub failed. Reason: {0}
datahub.disk.update.in.progress=Disk update on Data Hub is in progress.
datahub.disk.update.finished=Disk update on Data Hub successfully completed.
datahub.disk.update.failed=Disk update failed on Data Hub, please validate the input or retry the operation. Reason: {0}

cluster.disk.resize.failed=Unable to resize disks on stack {0}, because: {1}. Please validate the input or retry the operation.
cluster.disk.resize.started=Resizing disks for group {0} started.
cluster.disk.resize.complete=Resizing disks on stack complete.

authorization.doc=Please read documentation about user management in official doc: https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-managing-user-access.html. More specifically, you can read about roles here: https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-understanding-roles-resource-roles.html. If you need more detailed information about permissions, you need to check underlying service of user management.

conclusion.gateway.network.status.failed=Failed to check network status for nodes: {0}
conclusion.gateway.services.status.failed=Failed to check status of services for nodes: {0}
conclusion.services.check.failed=There are unhealthy services on nodes: {0}. Please check the instances on your cloud provider for further details.
conclusion.services.check.failed.details=There are unhealthy services on nodes: {0}
conclusion.salt.collect.unreachable.found=Unreachable nodes: {0}. We detected that cluster members can't communicate with each other. Please validate if all cluster members are available and healthy on your cloud provider.
conclusion.salt.collect.unreachable.found.details=Unreachable salt minions based on salt ping: {0}
conclusion.salt.collect.unreachable.failed=Collecting unreachable nodes failed. Please validate if all cluster members are available and healthy on your cloud provider.
conclusion.salt.collect.unreachable.failed.details=Collecting unreachable nodes failed, reason: {0}
conclusion.salt.master.services.unhealthy=There are unhealthy services on master node: {0}. Please check the instances on your cloud provider for further details.
conclusion.salt.master.services.unhealthy.details=Unhealthy services on master: {0}
conclusion.salt.minions.unreachable=Unreachable nodes: {0}. We detected that cluster members can't communicate with each other. Please validate if all cluster members are available and healthy on your cloud provider.
conclusion.salt.minions.unreachable.details=Unreachable salt minions based on node status service: {0}
conclusion.network.nginx.unreachable=Nginx is not reachable. The possible reasons are: CCM is not available or the network configuration is not CDP compatible. 
conclusion.network.ccm.not.accessible=CCM is not accessible from node {0}. Please check network settings.
conclusion.network.ccm.not.accessible.details=CCM health status is {0} for node {1}
conclusion.network.cloudera.com.not.accessible=cloudera.com is not accessible from node: {0}. Please check network settings.
conclusion.network.cloudera.com.not.accessible.details=cloudera.com accessibility status is {0} for node {1}
conclusion.network.neighbour.not.accessible=Node {0} cannot reach any neighbour nodes. Please check nodes and network settings.
conclusion.network.neighbour.not.accessible.details=Neighbours accessibility status is {0} for node {1}
conclusion.cm.server.unreachable=CM server is unreachable.
conclusion.cm.unhealthy.vms.found=Unhealthy and/or unknown VMs found based on CM status. Unhealthy VMs: {0}, unknown VMs: {1}
conclusion.cm.unhealthy.vms.found.details=Unhealthy VMs: {0}, unknown VMs: {1}
conclusion.provider.not.running.vms.found=Not running and/or unknown VMs found based on provider status. Not running VMs: {0}, unknown VMs: {1}
conclusion.provider.not.running.vms.found.details=Not running VMs: {0}, unknown VMs: {1}
cluster.changing.deletevolumes=Deleting attached volumes in the {0} group.
cluster.changing.deletevolumes.validation.start=Validating delete volumes request in the {0} group.
cluster.changing.deletevolumes.unmount.start=Starting unmounting of block storages in {0} group.
cluster.changing.deletevolumes.cmconfig.start=Starting CM Config modification for deleting volumes in {0} group.
cluster.changed.deletevolumes=Deleted attached volumes in the {0} group.
cluster.change.deletevolumes.failed=Unable to detach and delete volumes on the current cluster because: {0}.
cluster.change.deletevolumes.delete.failed=Unable to delete volumes in the {0} group.
cluster.changing.mount.volumes=Mounting {0} attached volumes in the {1} group.
cluster.change.cm.config=Updating CM configs for attached volumes in the {0} group.
cluster.changing.add.volumes=Adding {0} additional EBS volumes in the {1} group.
cluster.changed.add.volumes=Added {0} EBS volumes in the {1} group.
cluster.changing.attach.volumes=Attaching {0} additional EBS volumes in the {1} group.
cluster.add.volumes.failed=Unable to add volumes in the {0} group.
datalake.add.volumes.failed=Adding additional volumes on Data Lake failed.
datalake.add.volumes.in.progress=Adding additional volumes on Data Lake is in progress.
datalake.add.volumes.complete=Adding additional volumes on Data Lake is complete.