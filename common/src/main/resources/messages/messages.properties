test.message=Hi my dear friend
stack.image.setup=Setting up CDP image
stack.infrastructure.bootstrap=Bootstrapping cluster
stack.infrastructure.disk.mount=Mounting attached disks
stack.infrastructure.starting=Infrastructure is now starting
stack.infrastructure.started=Infrastructure successfully started
stack.infrastructure.stopping=Infrastructure is now stopping
stack.infrastructure.stopped=Infrastructure successfully stopped
stack.delete.in.progress=Terminating the cluster and its infrastructure
stack.pre.delete.in.progress=Pre-terminating the cluster and its infrastructure
stack.delete.completed=The cluster and its infrastructure have successfully been terminated
stack.forced.delete.completed=The cluster and its infrastructure have been force terminated in CloudBreak. You may need to terminate the cluster manually at the cloud provider.
stack.adding.instances=Adding {0} new instance(s) to host group: {1}. Adjustment type: {2}. Threshold: {3}
stack.gateway.certificate.create.skipped=The generation of valid certificate has been failed, installation of your cluster is continuing with a generated self-signed certificate.
stack.metadata.extend.with.count=Infrastructure metadata extension finished with failures, {0} new instances was created instead of {1}
stack.removing.instance=Removing instance from the infrastructure
stack.removing.instance.finished=Instance removed
stack.removing.instance.failed=Failed to remove instance
stack.mount.disks.on.new.hosts=Mounting disks on new nodes
stack.downscale.instances=Removing instance(s): {0}
stack.downscale.success=Removed instance(s): {0}
stack.downscale.failed=Cluster infrastructure downscale failed: {0}
stack.upscale.quota.issue=You have reached your quota limit on provider. Error message: {0}
stack.select.for.downscale=Selected node(s) for downscale: {0}
stack.stop.requested=Cluster infrastructure stop requested
stack.provisioning=Creating infrastructure
stack.infrastructure.time=Infrastructure creation took {0} seconds
stack.setup.time=Image preparation took {0} seconds
stack.setup.start=Image preparation started
stack.infrastructure.subnets.updating=Updating allowed subnets
stack.infrastructure.subnets.updated=Allowed subnets updated
stack.infrastructure.update.failed=Stack update failed. {0}
stack.infrastructure.create.failed=Infrastructure creation failed. Reason: {0}
stack.instance.metadata.restored=Repair failed for instance, metadata restored for host: {0}
stack.infrastructure.rollback=Infrastructure rollback. Reason: {0}
stack.infrastructure.rollback.failed=Infrastructure rollback failed. Reason: {0}
stack.infrastructure.delete.failed=Infrastructure termination failed. Reason: {0}
stack.infrastructure.start.failed=Infrastructure start failed. Reason: {0}
stack.infrastructure.stop.failed=Infrastructure stop failed. Reason: {0}
stack.repair.detection.started=Started stack repair. Detecting unhealthy instances.
stack.repair.attempting=Found unhealthy instances, attempting to repair stack.
stack.repair.complete.clean=Completed stack repair. No unhealthy instances found.
stack.repair.failed=Failed to repair cluster. Reason: {0}
stack.repair.triggered=Stack repair triggered.
stack.datalake.update=Update cluster due to changed datalake
stack.datalake.update.finished=Update cluster finished
stack.datalake.update.failed=Failed to update the cluster. {0}
stack.infrastructure.create.rollback=Could not create {0} instance(s), node count is decreased on group {1}. Reason: {2}
stack.cleanup.service.trigger.sync=Couldn't retrieve the cluster's status, starting to sync.

stack.diagnostics.salt.validation.running=Validating salt minion connectivity before initializing diagnostics collection. Esxclude hosts: {0}
stack.diagnostics.salt.validation.running.skip.unresponsive=Validating salt minion connectivity before initializing diagnostics collection (skip unresponsive hosts). Initially excluded hosts: {0}
stack.diagnostics.salt.pillar.update.running=Update telemetry related salt configurations.
stack.diagnostics.salt.state.update.running=Update telemetry related salt states.
stack.diagnostics.preflight.check.running=Running pre-flight checks.
stack.diagnostics.preflight.check.finished={0}Pre-flight {1} check result: {2}
stack.diagnostics.vm.preflight.check.running=Running pre-flight checks on VM nodes.
stack.diagnostics.telemetry.upgrade.running=Running on-the-fly telemetry upgrade.
stack.diagnostics.init.running=Diagnostics collection initialization in progress. Hosts: {0}, Excluded hosts: {1}, Host-groups: {2}
stack.diagnostics.ensure.machine.user=Check availability of UMS resources for diagnostics.
stack.diagnostics.collection.running=Diagnostics collection in progress. Destination for logs: {0}
stack.diagnostics.upload.running=Diagnostics uploading in progress. {0}
stack.diagnostics.cleanup.running=Diagnostics cleanup in progress
stack.diagnostics.collection.finished=Diagnostics collection finished
stack.diagnostics.collection.failed=Diagnostics collection failed. Reason: {0}
stack.cm.diagnostics.init.running=CM based diagnostics collection initialization in progress.
stack.cm.diagnostics.collection.running=CM based diagnostics collection in progress. Destination for diagnostics data: {0}
stack.cm.diagnostics.upload.running=CM based diagnostics uploading in progress. {0}
stack.cm.diagnostics.cleanup.running=CM based diagnostics cleanup in progress
stack.cm.diagnostics.collection.finished=CM based Diagnostics collection finished
stack.cm.diagnostics.collection.failed=CM based diagnostics collection failed. Reason: {0}
stack.cm.mixed.package.versions.failed=The current Cloudera Manager {0}, Runtime {1} combination is not certified by Cloudera therefore their usage is unadvised and might lead to unexpected errors. Please consider upgrading them to a certified combination, like {2}
stack.cm.mixed.package.versions.failed.no.candidate=The current Cloudera Manager {0}, Runtime {1} combination is not certified by Cloudera therefore their usage is unadvised and might lead to unexpected errors. Please contact the Cloudera support for further actions.
stack.cm.mixed.package.versions.failed.multiple=The current Cloudera Manager {0}, Runtime {1} combination is not certified by Cloudera therefore their usage is unadvised and might lead to unexpected errors. Please consider upgrading them to a certified combination, like {2}. For other possible combinations please visit the image catalog at {3}
stack.cm.mixed.package.versions.newer.failed=The current Cloudera Manager {0}, Runtime {1} combination is not certified by Cloudera therefore their usage is unadvised and might lead to unexpected errors. The following components are newer {2} then the expected versions {3}. Please use recovery or contact the Cloudera support.

cluster.starting=Starting cluster
cluster.dns.update.finished=DNS entries have been updated for the cluster
cluster.building=Installing CDP services
cluster.built=CDP services have been installed
recovery.finished=Cluster recovery has been completed
ambari.cluster.created=Cloudera Manager cluster created
cluster.started=Cluster has been started
cluster.stopping=Stopping cluster
cluster.stopped=Cluster stopped
cluster.termination.started=Cluster termination has been started
cluster.provision.idbrokerhost.cloudstorage.vmvalidation.failed=Validation of the cluster's cloud storage access for logging failed on the IDBroker host.

cluster.regenerate.keytabs=Regenerating kerberos keytabs on cluster
cluster.decommission.failed.force.delete.continue=Error during cluster decommission but force delete will continue. Reason: {0}
cluster.stop.management.server.started=Stopping cluster management server
cluster.start.management.server.started=Starting cluster management server
cluster.single.master.repair.reinstall.components=Reinstalling components on cluster management server
cluster.restart.all.started=Restarting all components on all hosts
cluster.start.components.started=Starting components
cluster.stop.components.started=Stopping all cluster components

cluster.certificate.reissue=Certificate renewal of the cluster has been started. Reissuing the certificate.
cluster.certificate.redeploy=The redeployment of the reissued certificate to the cluster has been started.
cluster.certificate.renewal.finished=Renewal of the cluster's certificate finished.
cluster.certificate.renewal.failed=Renewal of the cluster's certificate failed: '{0}'.

cluster.certificates.rotation.started=Certificates rotation of the cluster has been started.
cluster.host.certificates.rotation=Host certificates rotation of the cluster has been started.
cluster.certificates.rotation.finished=Rotation of the cluster's certificates have finished.
cluster.certificates.rotation.failed=Rotations of the cluster's certificates have failed: '{0}'.

cluster.manager.server.restarting=Restarting Cluster Manager Server
cluster.services.restarting=Restarting Cluster services

cluster.scaling.stopstart.upscale.init=Scaling up (via instance start) for host group: {0}. Starting {1} node(s)
cluster.scaling.stopstart.upscale.nodes.started=Started Instances: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.nodes.notstarted=Some instances did not reach the desired 'STARTED' state, and may need attention: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.inadequate.nodes=Could not find adequate nodes to upscale(start) in hostGroup: {0}. Additional nodes may need to be added to the cluster. DesiredCount={1}, UpscaleCount={2}
cluster.scaling.stopstart.upscale.commissioning=Commissioning services. Host group: {0}, InstanceCount={1}, hostname(s): [{2}]
cluster.scaling.stopstart.upscale.commissioning2=Commissioning services. Host group: {0}, startedInstanceCount={1}, hostname(s): [{2}] existingInstanceCount={3}, hostname(s): [{4}]
cluster.scaling.stopstart.upscale.couldnotcommission=Could not commission services on all nodes. Some nodes may need attention: instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.upscale.waiting.hoststart=Waiting for {0} cm-host(s) to start before commissioning services
cluster.scaling.stopstart.upscale.cmhostsstarted={0} cm-host(s) started
cluster.scaling.stopstart.upscale.start.failed=Failed while attempting to start instances. Some instances may need attention, and may be in STARTED state without services running. Refer to previous messages. Potential instances: instanceCount={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.upscale.commission.failed=Failed while attempting to commission services. Some instances may be STARTED with services not running. See previous events for list of started instances. Some hosts may need attention. instanceCount={0}, host(s): [{1}]
cluster.scaling.stopstart.upscale.finished=Scaled up (via instance start) host group: {0}. Instance details: count={1}, hostname(s): {2}
cluster.scaling.stopstart.upscale.failed=Failed to upscale (via instance start). Reason: {0}

cluster.scaling.stopstart.downscale.init=Scaling down (via instance stop) for host group: {0}
cluster.scaling.stopstart.downscale.starting=Scaling down (via instance stop) {0} node(s) from host group: {1}. Decommissioning services on hosts: [{2}]
cluster.scaling.stopstart.downscale.couldnotdecommission=Could not decommission services on all nodes. Some nodes may need attention: instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.downscale.enteringcmmaintmode=Putting {0} cm-host(s) into maintenance mode
cluster.scaling.stopstart.downscale.enteredcmmaintmode=Finished putting {0} cm-host(s) into maintenance mode
cluster.scaling.stopstart.downscale.nodes.stopping=Stopping {0} node(s) from host group {1}. Hosts: [{2}]
cluster.scaling.stopstart.downscale.nodes.stopped=Stopped Instances: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.downscale.nodes.notstopped=Some instances did not reach the desired 'STOPPED' state, and may need attention: count={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.downscale.decommission.failed=Failed while attempting to decommission services. Some nodes may need attention. instanceCount={0}, hostname(s): [{1}]
cluster.scaling.stopstart.downscale.stop.failed=Failed while attempting to stop instances. Some instances may be in decommissioned state on CM. See previous events for details on such instances. Some instances may need attention on the cloud provider. instanceCount={0}, instanceId(s): [{1}]
cluster.scaling.stopstart.downscale.finished=Scaled down (via instance stop) host group: {0}. Instance details: count={1}, instance(s): {2}
cluster.scaling.stopstart.downscale.failed=Failed to downscale (via instance stop). Reason: {0}

cluster.scaling.up=Scaling up host group: {0}
cluster.re.register.with.cluster.proxy=Re-registering with Cluster Proxy service
cluster.scaled.up=Scaled up host group: {0}
cluster.scaling.down=Scaling down host group: {0}
cluster.scaled.down=Scaled down host group: {0}
cluster.scaled.down.none=0 instance has been scaled down
cluster.run.containers=Starting cluster containers
cluster.run.services=Starting Cloudera Manager
cluster.reset=Resetting cluster
ambari.cluster.configuring.security=Configuring Cloudera Manager cluster security
ambari.cluster.configured.security=Cloudera Manager cluster security configured
cluster.start.requested=Cluster start requested
cluster.changing.credential=Changing cluster authentication
cluster.changed.credential=Cluster authentication changed
cluster.change.credentail.failed=Unable to change cluster manager password.
cluster.start.failed=Cluster could not be started. Reason: {0}
cluster.stop.failed=Cluster could not be stopped. Reason: {0}
cluster.create.failed={0}
ambari.cluster.configure.security.failed=Failed to enable security on Cloudera Manager cluster. Reason: {0}
cluster.scaling.failed=New node(s) could not be {0} the cluster. Reason {1}
cluster.scaling.partially.failed=Scaling up was not fully successful. Some of the new node(s) could not be {0} the cluster. Reason {1}
ambari.cluster.mr.smoke.failed=Warning: MapReduce smoke test failed, check your cluster''s configurations!
ambari.cluster.install.failed=Cluster installation failed to complete, please check the Cloudera Manager UI for more details. You can try to reinstall the cluster with a different cluster definition or fix the failures in Cloudera Manager and sync the cluster with Cloudbreak later.
ambari.cluster.upscale.failed=Cluster upscale failed to complete, please check the Cloudera Manager UI for more details. You can try to fix the failures in Cloudera Manager and sync the cluster with Cloudbreak later.
ambari.cluster.prepare.dekerberizing.failed=Prepare cluster to dekerberizing failed to complete, please check the Cloudera Manager UI for more details.
ambari.cluster.prepare.dekerberizing.error=The de-registration of Kerberos principals couldn''t be done. The Cloudera Manager server should run and be reachable by Cloudbreak.
ambari.cluster.disable.kerberos.failed=Cluster dekerberizing failed to complete, please check the Cloudera Manager UI for more details.
ambari.cluster.host.join.failed=Error while waiting for hosts to connect.
ambari.cluster.delete.completed=Cluster {0} was terminated.
ambari.cluster.delete.failed=Termination of cluster failed. Reason: {0}
ambari.cluster.services.init.failed=Initializing Cloudera Manager services failed.
ambari.regenerate.kerberos.keytabs.failed=Regenerating kerberos keytabs on Cloudera Manager failed.
ambari.cluster.services.start.failed=Starting Cloudera Manager services failed.
ambari.cluster.services.stop.failed=Stopping Cloudera Manager services failed.
cluster.ambari.cluster.services.starting=Starting Cloudera Manager services.
cluster.ambari.cluster.services.started=Cloudera Manager services have been started.
cluster.ambari.cluster.services.stopping=Stopping Cloudera Manager services.
cluster.ambari.cluster.services.stopped=Cloudera Manager services have been stopped.
cluster.autorecovery.requested.host=Cluster autorecovery requested for node {0}
cluster.autorecovery.requested.cluster=Cluster autorecovery requested, failed nodes: {0}
cluster.manualrecovery.requested=Cluster instance replacement requested on entities: {0}
cluster.manualrecovery.could.not.start=Cluster instance replacement requested on entities could not start for nodes due to the following reason: {0}
cluster.manualrecovery.no.nodes.to.recover=Cluster instance replacement requested on nodes which do not require recovery or recovery is not possible.
cluster.failednodes.reported.cluster=Cloudera Manager reported health issues with node(s): {0}
cluster.failednodes.reported.host=Cloudera Manager reported health issues with node {0}. Node could be UNHEALTHY or down. Please check CM UI for further information.
cluster.recoverednodes.reported.cluster=Cloudera Manager reported that node(s) {0} status became HEALTHY.
cluster.recoverednodes.reported.host=Cloudera Manager reported that node {0} status became HEALTHY.
cluster.pgw.unhealthy.sync.started=Cloudera Manager is unreachable, cluster sync started. Please check the instances on Cloud Provider side.
cluster.gateway.change=Starting primary gateway change
cluster.gateway.changed.successfully=Primary gateway successfully changed to {0}
cluster.gateway.change.failed=Primary gateway change failed. Reason: {0}
cluster.ambari.cluster.decommissioning.time=The decommission of the Data nodes will take approximately {0}.

cluster.externaldatabase.deletion.started=External database termination started.
cluster.externaldatabase.deletion.failed=External database termination failed.
cluster.externaldatabase.deletion.finished=External database termination finished.
cluster.externaldatabase.creation.started=External database creation started.
cluster.externaldatabase.creation.failed=External database creation failed.
cluster.externaldatabase.creation.finished=External database creation finished.
cluster.externaldatabase.start.commenced=External database start commenced.
cluster.externaldatabase.start.failed=External database start failed.
cluster.externaldatabase.start.finished=External database start finished.
cluster.externaldatabase.stop.commenced=External database stop commenced.
cluster.externaldatabase.stop.failed=External database stop failed.
cluster.externaldatabase.stop.finished=External database stop finished.

cluster.ccm.upgrade.tunnel.update=Cluster Connectivity Manager tunnel type update.
cluster.ccm.upgrade.push.salt.states=Cluster Connectivity Manager upgrade pushing Salt states.
cluster.ccm.upgrade.reconfigure.nginx=Cluster Connectivity Manager upgrade reconfiguring NGINX.
cluster.ccm.upgrade.register.clusterproxy=Cluster Connectivity Manager upgrade re-register hosts.
cluster.ccm.upgrade.health.check=Cluster Connectivity Manager health check.
cluster.ccm.upgrade.remove.agent=Cluster Connectivity Manager remove previous version's agent.
cluster.ccm.upgrade.deregister.agent=Cluster Connectivity Manager unregister previous version's agent.
cluster.ccm.upgrade.failed=Cluster Connectivity Manager upgrade failed.
cluster.ccm.upgrade.finished=Cluster Connectivity Manager upgrade finished.

cluster.kerberosconfig.validation.failed=Kerberos config validation failed with: {0}

cluster.cloudconfig.validation.failed=Cloud config validation failed with the following errors: {0}
cluster.provider.validation.warning=Cloud platform validation succeeded with the following warnings: {0}

cm.cluster.command.failed=Cloudera Manager command [{0}] failed, you can check the command here: {1}
cm.cluster.command.timeout=Cloudera Manager command [{0}] timed out, you can check the command here: {1}
cm.cluster.services.started=Cloudera Manager services have been started.
cm.cluster.services.starting=Starting Cloudera Manager services.
cm.cluster.services.restarting=Restarting Cloudera Manager services.
cm.cluster.services.stopping=Stopping Cloudera Manager services.
cm.cluster.services.stopped=Cloudera Manager services have been stopped.
cm.cluster.updating.remote.data.context=Cloudera Manager updating remote data context, started.
cm.cluster.updated.remote.data.context=Cloudera Manager updating remote data context, complete.
cm.cluster.securitygroup.too.strict=The security group of the Cloudera Manager node is probably too strict. Please check if all the necessary ports are reachable. Error message: {0}.
cm.cluster.service.deregister.failed=Failed to deregister service(s) from Cloudera Manager. This has to be done manually. It's possible that CM or the instance is not running. 

cluster.bootstrapper.error.nodes.failed=Bootstrap failed on {0} nodes. These nodes will be terminated
cluster.bootstrapper.error.invalide.nodecount=Invalid node count on instance group {0}; Cluster creation failed
cluster.bootstrapper.error.deleting.node=Deleting node: {0}. Decreasing the node count on instance group: {1}

cluster.start.ignored=Cluster start request ignored, cluster is already available
cluster.stop.ignored=Cluster stop request ignored, cluster is already stopped
cluster.host.status.updated=Host [name: {0}] state has been updated to: {1}
ambari.cluster.resetting.ambari.database=Resetting Cloudera Manager database
ambari.cluster.ambari.database.reset=Cloudera Manager database has been reset
ambari.cluster.restarting.ambari.server=Restarting Cloudera Manager server
ambari.cluster.restarting.ambari.agent=Restarting Cloudera Manager agents
ambari.cluster.ambari.server.restarted=Cloudera Manager server restarted
ambari.cluster.ambari.agent.restarted=Cloudera Manager agents restarted
cluster.removing.nodes=Removing {0} node(s)
cluster.force.removing.nodes=Removing forcefully {0} node(s)
cluster.removing.node.lost.node.decommission.aborted=Decommission commands for lost nodes {0} were stucked in Cloudera Manager twice, thus were aborted, removing those nodes forcefully.
ambari.cluster.adding.node.to.hostgroup=Adding {0} new host(s) to the host group {1}

cluster.single.master.repair.started=Started repairing Cloudera Manager on recovered master node
cluster.single.master.repair.finished=Finished repairing Cloudera Manager on recovered master node
ambari.cluster.single.master.repair.failed=Repairing Cloudera Manager on recovered master node failed

stack.instance.terminate=Terminating instance {0}
stack.instance.delete=Deleting node {0}. Decreasing the node count on instance group {1}"

recipes.execute=Execute recipes: {0}
cluster.recipes.upload=Upload recipes: {0}

cluster.ambari.cluster.could.not.sync=Cluster can''t be synchronized, status: {0}
cluster.ambari.cluster.synchronized=The cluster state synchronized with Cloudera Manager: {0}
cluster.sync.instance.different.packages=The following packages are installed with different versions on hosts: {0}
cluster.sync.instance.missing.package.versions=There are missing package versions on hosts: {0}
cluster.sync.instance.failedquery.packages=Query of package versions has been failed on hosts: {0}

stack.stop.ignored=Stop request ignored; cluster infrastructure is already stopped.
stack.start.ignored=Start request ignored; cluster infrastructure is already started.

stack.metadata.setup.billing.changed=Billing changed due to upscaling of cluster infrastructure

stack.scaling.host.deleted=Host {0} deleted
stack.scaling.host.delete.failed=Could not delete host {0} from Cloudera Manager
stack.scaling.host.not.found=Host {0} is not found in Cloudera Manager
stack.scaling.billing.changed=Billing changed due to downscaling of cluster infrastructure.
stack.scaling.terminating.host.from.hostgroup=Terminating host: {0} from host group: {1}



stack.sync.instance.status.retrieval.failed=Couldn''t retrieve the status of instance {0} from the cloud provider.
stack.sync.instance.status.couldnt.determine=The state of one or more instances couldn''t be determined. Try syncing later.
stack.sync.instance.operation.in.progress=An operation on one or more instances is in progress. Try syncing later.
stack.sync.instance.stopped.on.provider=Some instances were stopped on the cloud provider. Restart or terminate them and try syncing later.
stack.sync.host.deleted=Deleted host {0} from Cloudera Manager as it is marked as terminated by the cloud provider.
stack.sync.instance.removal.failed=Instance {0} is terminated but couldn''t remove host from Cloudera Manager because it still reports the host as healthy. Try syncing later.
stack.sync.host.updated=Host {0} state has been updated to: {1}.
stack.sync.instance.terminated=Instance {0} is marked as terminated by the cloud provider, but couldn''t delete the host from Cloudera Manager.
stack.sync.instance.deleted.cbmetadata=Deleted instance {0} from Cloudbreak metadata because it couldn''t be found on the cloud provider.
stack.sync.instance.deletedbyprovider.cbmetadata=Deleted instance {0} from Cloudbreak metadata because it was deleted by the cloud provider.
stack.sync.versions.from.cm.to.db.succeeded=Reading CM and active parcel versions from CM server succeeded: {0}
stack.sync.versions.from.cm.to.db.failed=Reading CM and active parcel versions from CM server has failure(s): {0}
stack.sync.instance.updated=Updated metadata of instance {0} to {1} as the cloud provider reported it as {1}.
stack.sync.instance.failed=Updated metadata of instance {0} to failed, because update was in progress, but instance isn''t member of the cluster.

datalake.upgrade=Upgrade process initiated, target image id is {0}.
datalake.upgrade.could.not.start=Upgrade process could not be started due to the following reason: {0}
datalake.ccm.upgrade=Cluster Connectivity Manager upgrade initiated.
datalake.ccm.upgrade.error.environment.notlatest=Invalid state: environment {0} is not upgraded to the latest Cluster Connectivity Manager yet.
datalake.ccm.upgrade.error.invalid.count=More than one Data Lake is found for environment {0}. This case is not handled yet.
datalake.ccm.upgrade.no.datalake=Environment {0} has no Data Lake present.
datalake.ccm.upgrade.already.upgraded=Data Lake is already at the latest Cluster Connectivity Manager version.
datalake.ccm.upgrade.not.upgradeable=Data Lake is not upgradeable.
datalake.ccm.upgrade.not.available=Data Lake is not in Available state for upgrade.
datalake.ccm.upgrade.failed=Data Lake upgrade to the latest Cluster Connectivity Manager version failed.
datalake.ccm.upgrade.inprogress=Data Lake upgrade to the latest Cluster Connectivity Manager is in progress.

datahub.ccm.upgrade.error.environment.notlatest=Invalid state: environment {0} is not upgraded to the latest Cluster Connectivity Manager yet.
datahub.ccm.upgrade.not.available=Data Hub is not in Available state for upgrade.
datahub.ccm.upgrade.already.upgraded=Data Hub is already at the latest Cluster Connectivity Manager version.
datahub.ccm.upgrade.not.upgradeable=Data Lake is not upgradeable.

cluster.manager.upgrade=Upgrading Cluster Manager.
cluster.manager.upgrade.not.needed=Upgrading Cluster Manager is not needed as its buildnumber is the same: {0}.
cluster.manager.upgrade.failed=Failed to upgrade Cluster Manager. Reason: {0}.
cluster.manager.upgrade.finished=Cluster Manager upgrade was successful. New version is {0}.
cluster.upgrade=Upgrading Cluster Runtime.
cluster.upgrade.not.needed=Upgrading Cluster Runtime is not needed as its buildnumber is the same: {0}.
cluster.upgrade.failed=Failed to upgrade Cluster Runtime. Reason: {0}.
cluster.upgrade.finished.newversion=Cluster Runtime upgrade was successful. New version is {0}.
cluster.upgrade.finished.noversion=Cluster upgrade was successful.
cluster.upgrade.download.parcel=Downloading the new parcel
cluster.upgrade.distribute.parcel=Distributing the new parcel
cluster.upgrade.activate.parcel=Activating the new parcel
cluster.upgrade.start.upgrade=Calling upgrade on Runtime
cluster.upgrade.start.post-upgrade=Calling post upgrade on Runtime
cluster.upgrade.validation.started=Cluster upgrade validation started
cluster.upgrade.validation.finished=Cluster upgrade validation finished
cluster.upgrade.validation.failed=Cluster upgrade validation failed: {0}
cluster.upgrade.validation.skipped=Cluster upgrade validation skipped. Reason: {0}

cluster.salt.update.started=Salt update has been started
cluster.salt.update.failed=Failed to update salt states
cluster.salt.update.finished=Salt successfully updated

cluster.pillar.config.update.started=Update of cluster configuration started.
cluster.pillar.config.update.finished=Cluster configuration successfully updated.
cluster.pillar.config.update.failed=Failed to update cluster configuration,

cluster.salt.passwordrotate.started=Salt password rotation started
cluster.salt.passwordrotate.finished=Salt password successfully rotated
cluster.salt.passwordrotate.failed=Salt password rotation failed with error: {0}

resource.blueprint.created=Blueprint created.
resource.blueprint.deleted=Blueprint deleted.
resource.sdx.created=Datalake cluster created.
resource.sdx.deleted=Datalake cluster deleted.
resource.sdx.deletionstarted=Started deletion of datalake {0}.
resource.sdx.deletionfinished=Finished deletion of datalake {0}.
resource.sdx.deletionfailed=Failed to delete datalake {0}.
resource.sdx.deletedonproviderside=Datalake cluster deleted on provider side.
resource.sdx.envwait=Datalake cluster is waiting for the environment.
resource.sdx.envfinished=Environment for the Sdx cluster has been arrived.
resource.sdx.repair.started=Datalake repair flow started.
resource.sdx.repair.finished=Datalake repair flow finished.
resource.sdx.repair.failed=Datalake repair flow failed.
resource.sdx.failed=Datalake cluster creation failed.
resource.sdx.rdsdeletionstarted=Datalake RDS deletion started
resource.sdx.rdsdeletionfailed=Datalake RDS deletion failed
resource.sdx.provisionstarted=Datalake cluster provision started.
resource.sdx.provisionfinished=Datalake cluster provision finished.
resource.sdx.rdsdeletionfinished=Datalake RDS deletion finished.
resource.sdx.rdscreationstarted=Datalake RDS creation started.
resource.sdx.rdscreationfailed=Datalake RDS creation failed.
resource.sdx.rdscreationfinished=Datalake RDS creation finished.
resource.sdx.rdsstartstarted=Datalake RDS start operatation started.
resource.sdx.rdsstartfailed=Datalake RDS start operation failed.
resource.sdx.rdsstartfinished=Datalake RDS start operation finished.
resource.sdx.rdsstopstarted=Datalake RDS stop operation started.
resource.sdx.rdsstopfailed=Datalake RDS stop operation failed.
resource.sdx.rdsstopfinished=Datalake RDS stop operation finished.
resource.sdx.start.started=Datalake start flow executed.
resource.sdx.start.finished=Datalake start flow finished.
resource.sdx.start.failed=Datalake start failed.
resource.sdx.stop.started=Datalake stop executed.
resource.sdx.stop.finished=Datalake stop finished.
resource.sdx.stop.failed=Datalake stop failed.
resource.sdx.detach.started=Datalake detach started. Original datalake renamed to {0}.
resource.sdx.detach.finished=Datalake detach finished.
resource.sdx.detach.deletion.failed=Deletion of detached datalake {0} failed. Please retry the failed step via the UI.
resource.sdx.update_lb_dns.finished=Update of load balancer DNS for datalake finished.
resource.sdx.update_lb_dns.failed=Update of load balancer DNS for datalake failed. Reason: {0}.
resource.sdx.validation.skipped=Datalake object storage validation skipped. Reason: {0}.
resource.sdx.change.image.started=Datalake change image flow started.
resource.sdx.datalake.upgrade.started=Datalake upgrade flow started.
resource.sdx.datalake.upgrade.finished=Datalake upgrade flow finished.
resource.sdx.datalake.upgrade.failed=Datalake upgrade flow failed.
resource.sdx.sync.failed=Datalake sync failed
resource.workspace.created=Workspace created. {0}
resource.workspace.deleted=Workspace deleted. {0}
resource.clustertemplate.created=Cluster template created.
resource.clustertemplate.deleted=Cluster template deleted.
resource.credential.created=Credential created.
resource.credential.deleted=Credential deleted.
resource.credential.modified=Credential modified.
resource.ldap.created=Ldap created.
resource.ldap.deleted=Ldap deleted.
resource.mpack.created=Management pack created.
resource.mpack.deleted=Management pack deleted.
resource.kubernetesconfig.created=Kubernetes config created.
resource.kubernetesconfig.modified=Kubernetes config modified.
resource.kubernetesconfig.deleted=Kubernetes config deleted.
resource.network.created=Network created.
resource.network.deleted=Network deleted.
resource.recipe.created=Recipe created.
resource.recipe.deleted=Recipe deleted.
resource.rdsconfig.created=Rds config created.
resource.rdsconfig.deleted=Rds config deleted.
resource.proxyconfig.created=Proxy config created.
resource.proxyconfig.deleted=Proxy config deleted.
resource.securitygroup.created=Security group created.
resource.securitygroup.deleted=Security group deleted.
resource.template.created=Template created.
resource.template.deleted=Template deleted.
resource.topology.created=Topology created.
resource.topology.deleted=Topology deleted.
resource.imagecatalog.created=Image catalog created.
resource.imagecatalog.deleted=Image catalog deleted.
resource.maintenancemode.enabled=Maintenance mode enabled.
resource.maintenancemode.disabled=Maintenance mode disabled.

stack.image.update.started=Changing stack image
stack.image.update.finished=Stack image change finished
stack.image.update.failed=Image change failed with message: {0}
stack.image.update.packages.different=Missing packages from image: [{0}] Different package versions on image: [{1}] 
stack.image.update.cloudplatform.different=New image platform(s) [{0}] and current Stack platform [{1}] is different.
stack.image.update.osversion.different=New image OS [{0}] and OS type [{1}] is different from current OS [{2}] and OS type [{3}]

maintenance.mode.validation.started=The validation of the repo and image settings has begun
maintenance.mode.validation.finished.warn=Validation finished with warnings: {0}
maintenance.mode.validation.finished.nowarn=Validation finished successfully.
maintenance.mode.validation.failed=Validation failed. Reason: {0}

environment.initialization.started=Environment initialization started
environment.initialization.failed=Environment initialization failed
environment.validation.started=Environment creation request validation started
environment.validation.failed=Environment creation request validation failed
environment.validation.skipped=Environment creation request validation skipped. Reason: {0}.
environment.storage.consumption.collection.scheduling.started=Storage consumption collection scheduling started for environment
environment.storage.consumption.collection.scheduling.failed=Storage consumption collection scheduling failed for environment
environment.network.creation.started=Network creation/registration started for environment
environment.network.creation.failed=Network creation/registration failed for environment
environment.publickey.creation.started=Public Key creation/registration started for environment
environment.publickey.creation.failed=Public Key creation/registration failed for environment
environment.resource.encryption.initialization.started=Resource Encryption initialization started for environment
environment.resource.encryption.initialization.failed=Resource Encryption initialization failed for environment
environment.freeipa.creation.started=FreeIPA creation/registration started for environment
environment.freeipa.creation.failed=FreeIPA creation/registration failed for environment
environment.creation.finished=Environment creation successfully finished
environment.creation.failed=Environment creation failed

environment.network.deletion.started=Network deletion/deregistration started
environment.publickey.deletion.started=Public Key deletion/deregistration started
environment.clusterdefinition.deletion.started=Cluster definition deletion started
environment.database.deletion.started=Database deletion/deregistration started
environment.freeipa.deletion.started=FreeIPA deletion/deregistration started
environment.storage.consumption.collection.unscheduling.started=Storage consumption collection unscheduling started
environment.xp.deletion.started=Experience deletion started
environment.idbroker.mappings.deletion.started=IDBroker mappings deletion/deregistration started
environment.s3guard.table.deletion.started=S3Guard table deletion/deregistration started
environment.ums.resource.deletion.started=Environment assigned UMS resources deletion started
environment.datahubs.deletion.started=Data Hub clusters deletion/deregistration started
environment.datalakes.deletion.started=Data Lake clusters deletion/deregistration started
environment.resource.encryption.deletion.started=Resource Encryption deletion started
environment.deletion.finished=Environment deletion successfully finished
environment.deletion.failed=Environment deletion failed

environment.stop.datahub.started=Datahub stopped for Environment
environment.stop.datahub.failed=Datahub stop failed for Environment
environment.stop.datalake.started=Datalake stopped for Environment
environment.stop.datalake.failed=Datalake stop failed for Environment
environment.stop.freeipa.started=Freeipa stopped for Environment
environment.stop.freeipa.failed=Freeipa stop failed for Environment

environment.stop.success=Environment successfully stopped
environment.stop.failed=Environment failed to stop

environment.start.datahub.started=Datahub started for Environment
environment.start.datahub.failed=Datahub start failed for Environment
environment.restart.datahub.started=Datahubs restart started for Environment
environment.restart.datahub.failed=Datahubs restart failed for Environment
environment.restart.datahub.finished=Datahubs restart finished for Environment
environment.start.datalake.started=Datalake started for Environment
environment.start.datalake.failed=Datalake start failed for Environment
environment.start.freeipa.started=Freeipa started for Environment
environment.start.freeipa.failed=Freeipa start failed for Environment
environment.start.syncusers.started=Synchronize users started for Environment
environment.start.syncusers.failed=Synchronize users failed for Environment

environment.sync.finished=Environment sync is finished and new status is found, the new status is {0}
environment.salt.passwordrotate.finished=Environment salt password rotation finished
environment.salt.passwordrotate.failed=Environment salt password rotation failed. {0}

environment.start.success=Environment successfully started
environment.start.failed=Environment failed to start

environment.loadbalancer.update.env.started=Environment updated for load balancer
environment.loadbalancer.update.env.failed=Load balancer update for environment failed
environment.loadbalancer.update.stack.started=Stacks updated for load balancer
environment.loadbalancer.update.stack.failed=Load balancer update for stacks failed
environment.loadbalancer.update.finished=Load balancer update finished
environment.loadbalancer.update.success=Load balancer update successful
environment.loadbalancer.update.failed=Load balancer update failed

environment.upgrade.ccm.validation.started=Validation for CCM upgrade started
environment.upgrade.ccm.validation.failed=Validation for CCM upgrade failed
environment.upgrade.ccm.freeipa.started=Upgrading CCM on FreeIPA started
environment.upgrade.ccm.freeipa.failed=Upgrading CCM on FreeIPA failed
environment.upgrade.ccm.tunnelupdate.started=Updating tunnel type started
environment.upgrade.ccm.tunnelupdate.failed=Updating tunnel type failed
environment.upgrade.ccm.datalake.started=Upgrading CCM on Data Lake started
environment.upgrade.ccm.datalake.failed=Upgrading CCM on Data Lake failed
environment.upgrade.ccm.datahub.started=Upgrading CCM on Data Hub clusters started
environment.upgrade.ccm.datahub.failed=Upgrading CCM on Data Hub clusters failed
environment.upgrade.ccm.rollback=Rolling back CCM upgrade on last failed component
environment.upgrade.ccm.finished=Upgrading CCM finished
environment.upgrade.ccm.failed=Upgrading CCM failed

credential.azure.interactive.created=Azure interactive credential app successfully created
credential.azure.interactive.failed=Azure interactive credential app creation failed
credential.azure.interactive.status=Azure interactive credential status

retry.flow.start=Retrying {0} operation from the failed step.
common.bad.request.notification.pattern=Error during operation: {0}

datalake.database.backup=Datalake database backup initiated.
datalake.database.backup.finished=Database backup was successful.
datalake.database.backup.failed=Failed to backup database. Reason: {0}.
datalake.database.backup.could.not.start=Database backup could not be started due to the following reason: {0}
datalake.database.restore=Datalake database restore initiated.
datalake.database.restore.finished=Datalake database restore is successful.
datalake.database.restore.failed=Failed to restore database. Reason: {0}.
datalake.database.restore.could.not.start=Database restore could not be started due to the following reason: {0}
datalake.backup.in.progress=Datalake backup in progress.
datalake.restore.in.progress=Datalake restore in progress.
datalake.backup.finished=Datalake backup is complete.
datalake.restore.finished=Datalake restore is complete.
datalake.backup.failed=Datalake backup failed. Reason: {0}
datalake.restore.failed=Datalake restore failed. Reason: {0}
datalake.cert.renewal.started=Datalake certificate renewal started.
datalake.cert.renewal.failed=Datalake certificate renewal failed. Reason: {0}
datalake.cert.renewal.finished=Datalake certificate renewal finished.

datalake.recovery.requested=Datalake upgrade recovery requested. Cluster will be terminated and re-launched with the original runtime.
datalake.recovery.finished=Datalake recovery is complete.
datalake.recovery.in.progress=Datalake recovery is in progress.
datalake.recovery.failed=Datalake recovery failed.
datalake.recovery.bringup.failed=Datalake stack recovery failed.
datalake.recovery.bringup.finished=Datalake stack successfully recovered, creating datalake cluster.
datalake.recovery.teardown.finished=Datalake stack tear-down for recovery completed successfully.
datalake.resize.triggered=Datalake resize initiated.

datalake.datahub.refresh.in.progress=Datahub refresh in progress
datalake.datahub.refresh.failed=Datahub refresh failed. Reason: {0}

environment.stack.config.updates.started=Environment configuration update of clusters started.
environment.stack.config.updates.finished=Environment configuration update of clusters finished.
environment.stack.config.updates.failed=Environment configuration update of clusters failed due to the following reason: {0}  

resource.sdx.cert.rotation.started=Datalake certificate rotation started.
resource.sdx.cert.rotation.failed=Datalake certificate rotation failed. Reason: {0}
resource.sdx.cert.rotation.finished=Datalake certificate rotation finished.

stack.lb.update.create.entity=Creating load balancer entities.
stack.lb.update.create.cloud.resource=Creating load balancer cloud resources.
stack.lb.update.collect.metadata=Collecting load balancer metadata.
stack.lb.update.register.public.dns=Registering load balancer public DNS.
stack.lb.update.register.freeipa.dns=Register load balancer FreeIPA DNS.
stack.lb.update.update.cm.config=Updating CM frontend URL.
stack.lb.update.restart.cm=Restarting the CM server.
stack.lb.update.finished=Load balancer creation is finished.
stack.lb.update.failed=Load balancer creation failed. Reason: {0}

aws.variant.migration.successful=Successfully migrated the cloud formation template. The CF template and autoscaling group were removed
aws.variant.migration.failed=Failed to migrate the cloud formation template

authorization.doc=Please read documentation about user management in official doc: https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-managing-user-access.html. More specifically, you can read about roles here: https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-understanding-roles-resource-roles.html. If you need more detailed information about permissions, you need to check underlying service of user management.
